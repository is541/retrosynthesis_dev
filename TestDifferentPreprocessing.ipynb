{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad2f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 602, I. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2616, [S-2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3219, N. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 4739, Br. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5022, S. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5961, [Mg]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 6642, F. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 12323, Cl. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 13190, O. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "/Users/ilariasartori/miniforge3/envs/syntheseus_temp/lib/python3.10/site-packages/deepchem/feat/base_classes.py:322: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37dec789d2f4ed49f4a0460f3493681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734fee606f714f729ca00a11f8e87c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn Model - Epoch 1/5, TrainLoss: 3.5433737258911133, ValLoss: 2.9942778423428535\n",
      "gnn Model - Epoch 5/5, TrainLoss: 1.280034483909607, ValLoss: 1.3215257059782743\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to train neural network to get molecules embeddings\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from paroutes import PaRoutesInventory, get_target_smiles\n",
    "from embedding_model import (\n",
    "    fingerprint_preprocess_input,\n",
    "    gnn_preprocess_input,\n",
    "    CustomDataset,\n",
    "    collate_fn,\n",
    "    # SampleData,\n",
    "    fingerprint_vect_from_smiles,\n",
    "    compute_embeddings,\n",
    "    GNNModel,\n",
    "    FingerprintModel,\n",
    "    NTXentLoss,\n",
    "    num_heavy_atoms\n",
    ")\n",
    "from paroutes import PaRoutesInventory\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from rdkit import Chem\n",
    "import deepchem as dc\n",
    "\n",
    "\n",
    "with open(f\"config_gnn_0627.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "experiment_name = config[\"experiment_name\"]\n",
    "checkpoint_folder = f\"GraphRuns/{experiment_name}/\"\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "checkpoint_name = \"checkpoint.pth\"\n",
    "\n",
    "# if not args.load_from_preprocessed_data:\n",
    "# Save config in output folder\n",
    "with open(f\"{checkpoint_folder}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# Read routes data\n",
    "input_file_routes = f'Runs/{config[\"run_id\"]}/targ_routes.pickle'\n",
    "# input_file_distances = f'Runs/{config[\"run_id\"]}/targ_to_purch_distances.pickle'\n",
    "\n",
    "# Routes data\n",
    "with open(input_file_routes, \"rb\") as handle:\n",
    "    targ_routes_dict = pickle.load(handle)\n",
    "\n",
    "# # Load distances data\n",
    "# with open(input_file_distances, 'rb') as handle:\n",
    "#     distances_dict = pickle.load(handle)\n",
    "\n",
    "# Inventory\n",
    "\n",
    "inventory = PaRoutesInventory(n=5)\n",
    "purch_smiles = [mol.smiles for mol in inventory.purchasable_mols()]\n",
    "# len(purch_smiles)\n",
    "\n",
    "# def num_heavy_atoms(mol):\n",
    "#     return Chem.rdchem.Mol.GetNumAtoms(mol, onlyExplicit=True)\n",
    "\n",
    "purch_mol_to_exclude = []\n",
    "purch_nr_heavy_atoms = {}\n",
    "for smiles in purch_smiles:\n",
    "    nr_heavy_atoms = num_heavy_atoms(Chem.MolFromSmiles(smiles))\n",
    "    if nr_heavy_atoms < 2:\n",
    "        purch_mol_to_exclude = purch_mol_to_exclude + [smiles]\n",
    "    purch_nr_heavy_atoms[smiles] = nr_heavy_atoms\n",
    "\n",
    "if config[\"run_id\"] == \"202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7\":\n",
    "    all_targets = get_target_smiles(n=5)\n",
    "elif config[\"run_id\"] == \"Guacamol_combined\":\n",
    "    with open(\"Data/Guacamol/guacamol_v1_test_10ksample.txt\", \"r\") as f:\n",
    "        all_targets = [line.strip() for line in f.readlines()]\n",
    "\n",
    "targ_route_not_in_route_dict = {}\n",
    "for target in all_targets:\n",
    "    targ_route_not_in_route_dict[target] = {}\n",
    "\n",
    "    target_routes_dict = targ_routes_dict.get(target, \"Target_Not_Solved\")\n",
    "\n",
    "    if target_routes_dict == \"Target_Not_Solved\":\n",
    "        purch_in_route = []\n",
    "    else:\n",
    "        target_route_df = target_routes_dict[\"route_1\"]\n",
    "        purch_in_route = list(\n",
    "            target_route_df.loc[target_route_df[\"label\"] != \"Target\", \"smiles\"]\n",
    "        )\n",
    "    #         purch_in_route = [smiles for smiles in purch_in_route if smiles in purch_smiles]\n",
    "    purch_not_in_route = [\n",
    "        purch_smile\n",
    "        for purch_smile in purch_smiles\n",
    "        if purch_smile not in purch_in_route\n",
    "    ]\n",
    "    random.seed(config[\"seed\"])\n",
    "\n",
    "    \n",
    "    if config[\"neg_sampling\"] == \"uniform\":\n",
    "        purch_not_in_route_sample = random.sample(\n",
    "            purch_not_in_route, config[\"not_in_route_sample_size\"]\n",
    "        )\n",
    "    elif config[\"neg_sampling\"] == \"...\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f'{config[\"neg_sampling\"]}')\n",
    "\n",
    "    # Filter out molecules with only one atom (problems with featurizer)\n",
    "    purch_in_route = [\n",
    "        smiles for smiles in purch_in_route if smiles not in purch_mol_to_exclude\n",
    "    ]\n",
    "    purch_not_in_route_sample = [\n",
    "        smiles\n",
    "        for smiles in purch_not_in_route_sample\n",
    "        if smiles not in purch_mol_to_exclude\n",
    "    ]\n",
    "\n",
    "    targ_route_not_in_route_dict[target][\"positive_samples\"] = purch_in_route\n",
    "    targ_route_not_in_route_dict[target][\n",
    "        \"negative_samples\"\n",
    "    ] = purch_not_in_route_sample\n",
    "\n",
    "# Get a random sample of keys from targ_routes_dict\n",
    "if config[\"nr_sample_targets\"] != -1:\n",
    "    sample_targets = random.sample(\n",
    "        list(targ_route_not_in_route_dict.keys()), config[\"nr_sample_targets\"]\n",
    "    )\n",
    "else:\n",
    "    sample_targets = targ_route_not_in_route_dict\n",
    "# Create targ_routes_dict_sample with the sampled keys and their corresponding values\n",
    "targ_route_not_in_route_dict_sample = {\n",
    "    target: targ_route_not_in_route_dict[target] for target in sample_targets\n",
    "}\n",
    "\n",
    "input_data = targ_route_not_in_route_dict_sample\n",
    "\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "\n",
    "    purch_mols = [Chem.MolFromSmiles(smiles) for smiles in purch_smiles]\n",
    "    purch_featurizer = featurizer.featurize(purch_mols)\n",
    "    purch_featurizer_dict = dict(zip(purch_smiles, purch_featurizer))\n",
    "    with open(f\"{checkpoint_folder}/purch_featurizer_dict.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(purch_featurizer_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    fingerprint_num_atoms_dict = None\n",
    "\n",
    "    dataset = gnn_preprocess_input(\n",
    "        input_data=input_data, \n",
    "        featurizer=featurizer, \n",
    "        featurizer_dict=purch_featurizer_dict,\n",
    "        pos_sampling=config[\"pos_sampling\"],\n",
    "    )\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    purch_fingerprints = list(map(fingerprint_vect_from_smiles, purch_smiles))\n",
    "    purch_fingerprints_dict = dict(zip(purch_smiles, purch_fingerprints))\n",
    "    with open(\n",
    "        f\"{checkpoint_folder}/purch_fingerprints_dict.pickle\", \"wb\"\n",
    "    ) as handle:\n",
    "        pickle.dump(\n",
    "            purch_fingerprints_dict, handle, protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "    # Also save dict to retrieve number of atoms from fingerprints\n",
    "    # fingerprint_num_atoms_dict = {\n",
    "    #     torch.tensor(fp, dtype=torch.double): purch_nr_heavy_atoms[smiles]\n",
    "    #     for smiles, fp in purch_fingerprints_dict.items()\n",
    "    # }\n",
    "    # with open(\n",
    "    #     f\"{checkpoint_folder}/fingerprint_num_atoms_dict.pickle\", \"wb\"\n",
    "    # ) as handle:\n",
    "    #     pickle.dump(\n",
    "    #         fingerprint_num_atoms_dict, handle, protocol=pickle.HIGHEST_PROTOCOL\n",
    "    #     )\n",
    "\n",
    "    dataset = fingerprint_preprocess_input(\n",
    "        input_data, \n",
    "        fingerprints_dict=purch_fingerprints_dict, \n",
    "        pos_sampling=config[\"pos_sampling\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "# if args.save_preprocessed_data:\n",
    "#     with open(f\"{checkpoint_folder}/preprocessed_targets.pickle\", \"wb\") as handle:\n",
    "#         pickle.dump(preprocessed_targets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(\n",
    "#         f\"{checkpoint_folder}/preprocessed_positive_samples.pickle\", \"wb\"\n",
    "#     ) as handle:\n",
    "#         pickle.dump(\n",
    "#             preprocessed_positive_samples, handle, protocol=pickle.HIGHEST_PROTOCOL\n",
    "#         )\n",
    "#     with open(\n",
    "#         f\"{checkpoint_folder}/preprocessed_negative_samples.pickle\", \"wb\"\n",
    "#     ) as handle:\n",
    "#         pickle.dump(\n",
    "#             preprocessed_negative_samples, handle, protocol=pickle.HIGHEST_PROTOCOL\n",
    "#         )\n",
    "# else:\n",
    "#     with open(f\"{checkpoint_folder}/preprocessed_targets.pickle\", \"rb\") as handle:\n",
    "#         preprocessed_targets = pickle.load(handle)\n",
    "#     with open(f\"{checkpoint_folder}/preprocessed_positive_samples.pickle\", \"rb\") as handle:\n",
    "#         preprocessed_positive_samples = pickle.load(handle)\n",
    "#     with open(f\"{checkpoint_folder}/preprocessed_negative_samples.pickle\", \"rb\") as handle:\n",
    "#         preprocessed_negative_samples = pickle.load(handle)\n",
    "#     if config[\"model_type\"] == \"fingerprints\":\n",
    "#         with open(\n",
    "#             f\"{checkpoint_folder}/fingerprint_num_atoms_dict.pickle\", \"rb\"\n",
    "#         ) as handle:\n",
    "#             fingerprint_num_atoms_dict = pickle.load(handle)\n",
    "#     else:\n",
    "#         fingerprint_num_atoms_dict = None\n",
    "\n",
    "#     dataset = CustomDataset(\n",
    "#             preprocessed_targets,\n",
    "#             preprocessed_positive_samples,\n",
    "#             preprocessed_negative_samples,\n",
    "#         )\n",
    "\n",
    "# Train validation split\n",
    "validation_ratio = config[\"validation_ratio\"]\n",
    "num_samples = len(dataset)\n",
    "num_val_samples = int(validation_ratio * num_samples)\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(num_samples), test_size=num_val_samples, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    shuffle=config[\"train_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"val_batch_size\"],\n",
    "    shuffle=config[\"val_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# Batch size: The batch size determines the number of samples processed in each iteration during training or validation. In most cases, it is common to use the same batch size for both training and validation to maintain consistency. However, there are situations where you might choose a different batch size for validation. For instance, if memory constraints are more relaxed during validation, you can use a larger batch size to speed up evaluation.\n",
    "# Shuffle training data: Shuffling the training data before each epoch is beneficial because it helps the model see the data in different orders, reducing the risk of the model learning patterns specific to the order of the data. Shuffling the training data introduces randomness and promotes better generalization.\n",
    "# No shuffle for validation data: It is generally not necessary to shuffle the validation data because validation is meant to evaluate the model's performance on unseen data that is representative of the real-world scenarios. Shuffling the validation data could lead to inconsistent evaluation results between different validation iterations, making it harder to track the model's progress and compare performance.\n",
    "\n",
    "# Define network dimensions\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    gnn_input_dim = dataset.targets[0].node_features.shape[1]\n",
    "    gnn_hidden_dim = config[\"hidden_dim\"]\n",
    "    gnn_output_dim = config[\"output_dim\"]\n",
    "\n",
    "    with open(f\"{checkpoint_folder}/input_dim.pickle\", \"wb\") as f:\n",
    "        pickle.dump({\"input_dim\": gnn_input_dim}, f)\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    #     fingerprint_input_dim = preprocessed_targets[0].GetNumBits()\n",
    "    fingerprint_input_dim = dataset.targets[0].size()[\n",
    "        0\n",
    "    ]  # len(preprocessed_targets[0].node_features)\n",
    "    fingerprint_hidden_dim = config[\"hidden_dim\"]\n",
    "    fingerprint_output_dim = config[\"output_dim\"]\n",
    "\n",
    "    with open(f\"{checkpoint_folder}/input_dim.pickle\", \"wb\") as f:\n",
    "        pickle.dump({\"input_dim\": fingerprint_input_dim}, f)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "# Step 3: Set up the training loop for the GNN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    model = GNNModel(\n",
    "        input_dim=gnn_input_dim,\n",
    "        hidden_dim=gnn_hidden_dim,\n",
    "        output_dim=gnn_output_dim,\n",
    "    ).to(device)\n",
    "    model.double()\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    model = FingerprintModel(\n",
    "        input_dim=fingerprint_input_dim,\n",
    "        hidden_dim=fingerprint_hidden_dim,\n",
    "        output_dim=fingerprint_output_dim,\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "loss_fn = NTXentLoss(temperature=config[\"temperature\"], device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "\n",
    "load_from_checkpoint = False\n",
    "# input_checkpoint_folder  = 'GraphRuns/gnn_0629'\n",
    "# input_checkpoint_path = f'{checkpoint_folder}/epoch_71_checkpoint.pth'\n",
    "\n",
    "# STEP 5: Train loop\n",
    "# Check if a checkpoint exists and load the model state and optimizer state if available\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(input_checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    epoch_loss = pd.read_csv(f'{input_checkpoint_folder}/train_val_loss.csv')\n",
    "    best_val_loss = epoch_loss[\"ValLoss\"].min()\n",
    "    with open(f'{input_checkpoint_folder}/model_min_val.pkl', \"rb\") as handle:\n",
    "        best_model = pickle.load(handle)\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    epoch_loss = pd.DataFrame(columns=[\"Epoch\", \"TrainLoss\", \"ValLoss\"])\n",
    "\n",
    "# Create a SummaryWriter for TensorBoard logging\n",
    "log_dir = (\n",
    "    f\"{checkpoint_folder}/logs\"  # Specify the directory to store TensorBoard logs\n",
    ")\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_model = None\n",
    "\n",
    "# epoch_loss = pd.DataFrame(columns=[\"Epoch\", \"TrainLoss\", \"ValLoss\"])\n",
    "for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute embeddings\n",
    "        embeddings_dataset = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model,\n",
    "            batch_data=batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        loss = loss_fn(embeddings_dataset)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total loss\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for val_batch_idx, val_batch_data in enumerate(val_data_loader):\n",
    "            # Compute embeddings\n",
    "            val_embeddings = compute_embeddings(\n",
    "                device=device,\n",
    "                model_type=config['model_type'],\n",
    "                model=model,\n",
    "                batch_data=val_batch_data,\n",
    "            )\n",
    "            # Compute loss\n",
    "            val_batch_loss = loss_fn(val_embeddings)\n",
    "\n",
    "            val_loss += val_batch_loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    # METRICS\n",
    "    # - TRAIN\n",
    "    # Compute average loss for the epoch\n",
    "    average_train_loss = train_loss / train_batches\n",
    "\n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", average_train_loss, epoch + 1)\n",
    "\n",
    "    # - VALIDATION\n",
    "    average_val_loss = val_loss / val_batches\n",
    "\n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/val\", average_val_loss, epoch + 1)\n",
    "\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": [epoch],\n",
    "            \"TrainLoss\": [average_train_loss],\n",
    "            \"ValLoss\": [average_val_loss],\n",
    "        }\n",
    "    )\n",
    "    epoch_loss = pd.concat([epoch_loss, new_row], axis=0)\n",
    "\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_model = model\n",
    "\n",
    "    if (epoch % 10 == 0) | (epoch == num_epochs - 1):\n",
    "        print(\n",
    "            f\"{config['model_type']} Model - Epoch {epoch+1}/{num_epochs}, TrainLoss: {average_train_loss}, ValLoss: {average_val_loss}\"\n",
    "        )\n",
    "\n",
    "        # Save the model and optimizer state as a checkpoint\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint_path = f\"{checkpoint_folder}/epoch_{epoch+1}_{checkpoint_name}\"  # Specify the checkpoint file path\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "        #         loss_df = pd.DataFrame({'Epoch': range(len(epoch_loss)), 'TrainLoss': epoch_loss})\n",
    "        epoch_loss.to_csv(f\"{checkpoint_folder}/train_val_loss.csv\", index=False)\n",
    "\n",
    "        # Save the best model as a pickle\n",
    "        best_model_path = (\n",
    "            f\"{checkpoint_folder}/model_min_val.pkl\"  #'path/to/best_model.pkl'\n",
    "        )\n",
    "\n",
    "        with open(best_model_path, \"wb\") as f:\n",
    "            pickle.dump(best_model, f)\n",
    "\n",
    "\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "# STEP 6: Plot\n",
    "\n",
    "# fig = px.line(x=epoch_loss['Epoch'], y=epoch_loss['TrainLoss'], title=\"Train loss\")\n",
    "# fig.update_layout(width=1000, height=600, showlegend=False)\n",
    "# fig.write_image(f\"{checkpoint_folder}/Train_loss.pdf\")\n",
    "# fig.show()\n",
    "\n",
    "# Create a new figure with two lines    \n",
    "fig = px.line()\n",
    "\n",
    "# Add the TrainLoss line to the figure\n",
    "fig.add_scatter(x=epoch_loss[\"Epoch\"], y=epoch_loss[\"TrainLoss\"], name=\"Train Loss\")\n",
    "\n",
    "# Add the ValLoss line to the figure\n",
    "fig.add_scatter(\n",
    "    x=epoch_loss[\"Epoch\"], y=epoch_loss[\"ValLoss\"], name=\"Validation Loss\"\n",
    ")\n",
    "\n",
    "# Set the title of the figure\n",
    "fig.update_layout(title=\"Train and Validation Loss\")\n",
    "\n",
    "# Set the layout size and show the legend\n",
    "fig.update_layout(width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "fig.write_image(f\"{checkpoint_folder}/Train_and_Val_loss.pdf\")\n",
    "time.sleep(10)\n",
    "fig.write_image(f\"{checkpoint_folder}/Train_and_Val_loss.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0873ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979d2d1cb807483bb2d312149bc088dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  1.4095381144434214\n"
     ]
    }
   ],
   "source": [
    "val_loss_post = 0.0\n",
    "val_batches_post = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for val_batch_idx, val_batch_data in tqdm(enumerate(val_data_loader)):\n",
    "        # Compute embeddings\n",
    "        val_embeddings_post = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model,\n",
    "            batch_data=val_batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        val_batch_loss_post = loss_fn(val_embeddings_post)\n",
    "\n",
    "        val_loss_post += val_batch_loss_post.item()\n",
    "        val_batches_post += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    average_val_loss_post = val_loss_post / val_batches_post\n",
    "\n",
    "print(\"Validation loss: \", average_val_loss_post)\n",
    "\n",
    "# Should be the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c69694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TO EVALUATE\n",
    "experiment_name = config[\"experiment_name\"]  # gnn_0627\n",
    "checkpoint_folder = f\"GraphRuns/{experiment_name}/\"\n",
    "input_checkpoint_name = f\"epoch_5_checkpoint.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6368d8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. LOAD MODEL \n",
    "# Option 1: FROM CHECKPOINT\n",
    "input_checkpoint_folder  = f'GraphRuns/{experiment_name}'\n",
    "input_checkpoint_path = f'{input_checkpoint_folder}/{input_checkpoint_name}'\n",
    "\n",
    "\n",
    "with open(f'{checkpoint_folder}/input_dim.pickle', \"rb\") as f:\n",
    "    input_dim = pickle.load(f)['input_dim']\n",
    "\n",
    "# Define network dimensions\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    gnn_input_dim = input_dim\n",
    "    gnn_hidden_dim = config[\"hidden_dim\"]\n",
    "    gnn_output_dim = config[\"output_dim\"]\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    #     fingerprint_input_dim = preprocessed_targets[0].GetNumBits()\n",
    "    fingerprint_input_dim = input_dim\n",
    "    fingerprint_hidden_dim = config[\"hidden_dim\"]\n",
    "    fingerprint_output_dim = config[\"output_dim\"]\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    model_loaded = GNNModel(\n",
    "        input_dim=gnn_input_dim,\n",
    "        hidden_dim=gnn_hidden_dim,\n",
    "        output_dim=gnn_output_dim,\n",
    "    ).to(device)\n",
    "    model_loaded.double()\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    model_loaded = FingerprintModel(\n",
    "        input_dim=fingerprint_input_dim,\n",
    "        hidden_dim=fingerprint_hidden_dim,\n",
    "        output_dim=fingerprint_output_dim,\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "loss_fn_loaded = NTXentLoss(temperature=config[\"temperature\"], device=device)\n",
    "\n",
    "checkpoint = torch.load(input_checkpoint_path)\n",
    "model_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# # # OPTION 2: From pickle\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loss_fn = NTXentLoss(temperature=config[\"temperature\"], device=device)\n",
    "# with open(f'{checkpoint_folder}/model_min_val.pkl', \"rb\") as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72163431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904b8c915265498cbd9f59c5fd8eb38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  1.295512031763792\n"
     ]
    }
   ],
   "source": [
    "val_loss1 = 0.0\n",
    "val_batches1 = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for val_batch_idx, val_batch_data in tqdm(enumerate(val_data_loader)):\n",
    "        # Compute embeddings\n",
    "        val_embeddings1_loaded = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model_loaded,\n",
    "            batch_data=val_batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        val_batch_loss1 = loss_fn_loaded(val_embeddings1_loaded)\n",
    "\n",
    "        val_loss1 += val_batch_loss1.item()\n",
    "        val_batches1 += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    average_val_loss1 = val_loss1 / val_batches1\n",
    "\n",
    "print(\"Validation loss: \", average_val_loss1)\n",
    "\n",
    "# If it is not the same, there is an issue in the save\n",
    "# If it is the same, there is an issue in the preprocessing \n",
    "# --> Try applying model and model_loaded (on the new preprocessed data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e399b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 602, I. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2616, [S-2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3219, N. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 4739, Br. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5022, S. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5961, [Mg]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 6642, F. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 12323, Cl. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 13190, O. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "/Users/ilariasartori/miniforge3/envs/syntheseus_temp/lib/python3.10/site-packages/deepchem/feat/base_classes.py:322: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50596bf89c0a4ce8b8fbc5f726048dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. PREPROCESS DATA AGAIN (Only gnn_preprocess_input)\n",
    "input_data_1 = input_data.copy()\n",
    "\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    featurizer_1 = dc.feat.MolGraphConvFeaturizer()\n",
    "\n",
    "    purch_mols_1 = [Chem.MolFromSmiles(smiles) for smiles in purch_smiles]\n",
    "    purch_featurizer_1 = featurizer.featurize(purch_mols_1)\n",
    "    purch_featurizer_dict_1 = dict(zip(purch_smiles, purch_featurizer_1))\n",
    "\n",
    "    dataset_1 = gnn_preprocess_input(\n",
    "        input_data=input_data_1, \n",
    "        featurizer=featurizer_1, \n",
    "        featurizer_dict=purch_featurizer_dict_1,\n",
    "        pos_sampling=config[\"pos_sampling\"],\n",
    "    )\n",
    "\n",
    "# elif config[\"model_type\"] == \"fingerprints\":\n",
    "#     purch_fingerprints = list(map(fingerprint_vect_from_smiles, purch_smiles))\n",
    "#     purch_fingerprints_dict = dict(zip(purch_smiles, purch_fingerprints))\n",
    "\n",
    "\n",
    "#     dataset = fingerprint_preprocess_input(\n",
    "#         input_data, \n",
    "#         fingerprints_dict=purch_fingerprints_dict, \n",
    "#         pos_sampling=config[\"pos_sampling\"],\n",
    "#     )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "\n",
    "# 2. TRAIN VALIDATION SPLIT\n",
    "validation_ratio = config[\"validation_ratio\"]\n",
    "num_samples_1 = len(dataset_1)\n",
    "num_val_samples_1 = int(validation_ratio * num_samples_1)\n",
    "\n",
    "train_indices_1, val_indices_1 = train_test_split(\n",
    "    range(num_samples_1), test_size=num_val_samples_1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset_1 = Subset(dataset_1, train_indices_1)\n",
    "val_dataset_1 = Subset(dataset_1, val_indices_1)\n",
    "\n",
    "train_data_loader_1 = DataLoader(\n",
    "    train_dataset_1,\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    shuffle=config[\"train_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_data_loader_1 = DataLoader(\n",
    "    val_dataset_1,\n",
    "    batch_size=config[\"val_batch_size\"],\n",
    "    shuffle=config[\"val_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# Batch size: The batch size determines the number of samples processed in each iteration during training or validation. In most cases, it is common to use the same batch size for both training and validation to maintain consistency. However, there are situations where you might choose a different batch size for validation. For instance, if memory constraints are more relaxed during validation, you can use a larger batch size to speed up evaluation.\n",
    "# Shuffle training data: Shuffling the training data before each epoch is beneficial because it helps the model see the data in different orders, reducing the risk of the model learning patterns specific to the order of the data. Shuffling the training data introduces randomness and promotes better generalization.\n",
    "# No shuffle for validation data: It is generally not necessary to shuffle the validation data because validation is meant to evaluate the model's performance on unseen data that is representative of the real-world scenarios. Shuffling the validation data could lead to inconsistent evaluation results between different validation iterations, making it harder to track the model's progress and compare performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb86e170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797307fb47be4c9ab509e8cbf8cb1a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  1.3687338531017303\n"
     ]
    }
   ],
   "source": [
    "val_loss_new_preprocess = 0.0\n",
    "val_batches_new_preprocess = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for val_batch_idx, val_batch_data in tqdm(enumerate(val_data_loader_1)):\n",
    "        # Compute embeddings\n",
    "        val_embeddings_new_preprocess = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model,\n",
    "            batch_data=val_batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        val_batch_loss_new_preprocess = loss_fn_loaded(val_embeddings_new_preprocess)\n",
    "\n",
    "        val_loss_new_preprocess += val_batch_loss_new_preprocess.item()\n",
    "        val_batches_new_preprocess += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    average_val_loss_new_preprocess = val_loss_new_preprocess / val_batches_new_preprocess\n",
    "\n",
    "print(\"Validation loss: \", average_val_loss_new_preprocess)\n",
    "\n",
    "# If it is not the same, there is an issue in the save\n",
    "# If it is the same, there is an issue in the preprocessing \n",
    "# --> Try applying model and model_loaded (on the new preprocessed data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb5610b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 602, I. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2616, [S-2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3219, N. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 4739, Br. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5022, S. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 5961, [Mg]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 6642, F. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 12323, Cl. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 13190, O. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "/Users/ilariasartori/miniforge3/envs/syntheseus_temp/lib/python3.10/site-packages/deepchem/feat/base_classes.py:322: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9439dcaabff4e97a2bf5c426ac5d567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read routes data\n",
    "input_file_routes = f'Runs/{config[\"run_id\"]}/targ_routes.pickle'\n",
    "# input_file_distances = f'Runs/{config[\"run_id\"]}/targ_to_purch_distances.pickle'\n",
    "\n",
    "# Routes data\n",
    "with open(input_file_routes, \"rb\") as handle:\n",
    "    targ_routes_dict = pickle.load(handle)\n",
    "\n",
    "# # Load distances data\n",
    "# with open(input_file_distances, 'rb') as handle:\n",
    "#     distances_dict = pickle.load(handle)\n",
    "\n",
    "# Inventory\n",
    "\n",
    "inventory = PaRoutesInventory(n=5)\n",
    "purch_smiles = [mol.smiles for mol in inventory.purchasable_mols()]\n",
    "# len(purch_smiles)\n",
    "\n",
    "# def num_heavy_atoms(mol):\n",
    "#     return Chem.rdchem.Mol.GetNumAtoms(mol, onlyExplicit=True)\n",
    "\n",
    "purch_mol_to_exclude = []\n",
    "purch_nr_heavy_atoms = {}\n",
    "for smiles in purch_smiles:\n",
    "    nr_heavy_atoms = num_heavy_atoms(Chem.MolFromSmiles(smiles))\n",
    "    if nr_heavy_atoms < 2:\n",
    "        purch_mol_to_exclude = purch_mol_to_exclude + [smiles]\n",
    "    purch_nr_heavy_atoms[smiles] = nr_heavy_atoms\n",
    "\n",
    "if config[\"run_id\"] == \"202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7\":\n",
    "    all_targets = get_target_smiles(n=5)\n",
    "elif config[\"run_id\"] == \"Guacamol_combined\":\n",
    "    with open(\"Data/Guacamol/guacamol_v1_test_10ksample.txt\", \"r\") as f:\n",
    "        all_targets = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "\n",
    "targ_route_not_in_route_dict_NEW = {}\n",
    "for target in all_targets:\n",
    "    targ_route_not_in_route_dict_NEW[target] = {}\n",
    "\n",
    "    target_routes_dict = targ_routes_dict.get(target, \"Target_Not_Solved\")\n",
    "\n",
    "    if target_routes_dict == \"Target_Not_Solved\":\n",
    "        purch_in_route = []\n",
    "    else:\n",
    "        target_route_df = target_routes_dict[\"route_1\"]\n",
    "        purch_in_route = list(\n",
    "            target_route_df.loc[target_route_df[\"label\"] != \"Target\", \"smiles\"]\n",
    "        )\n",
    "    #         purch_in_route = [smiles for smiles in purch_in_route if smiles in purch_smiles]\n",
    "    purch_not_in_route = [\n",
    "        purch_smile\n",
    "        for purch_smile in purch_smiles\n",
    "        if purch_smile not in purch_in_route\n",
    "    ]\n",
    "    random.seed(config[\"seed\"])\n",
    "\n",
    "    random.seed(9001)\n",
    "    if config[\"neg_sampling\"] == \"uniform\":\n",
    "        purch_not_in_route_sample = random.sample(\n",
    "            purch_not_in_route, config[\"not_in_route_sample_size\"]\n",
    "        )\n",
    "    elif config[\"neg_sampling\"] == \"...\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f'{config[\"neg_sampling\"]}')\n",
    "\n",
    "    # Filter out molecules with only one atom (problems with featurizer)\n",
    "    purch_in_route = [\n",
    "        smiles for smiles in purch_in_route if smiles not in purch_mol_to_exclude\n",
    "    ]\n",
    "    purch_not_in_route_sample = [\n",
    "        smiles\n",
    "        for smiles in purch_not_in_route_sample\n",
    "        if smiles not in purch_mol_to_exclude\n",
    "    ]\n",
    "\n",
    "    targ_route_not_in_route_dict_NEW[target][\"positive_samples\"] = purch_in_route\n",
    "    targ_route_not_in_route_dict_NEW[target][\n",
    "        \"negative_samples\"\n",
    "    ] = purch_not_in_route_sample\n",
    "\n",
    "# # Get a random sample of keys from targ_routes_dict\n",
    "# if config[\"nr_sample_targets\"] != -1:\n",
    "#     sample_targets = random.sample(\n",
    "#         list(targ_route_not_in_route_dict_NEW.keys()), config[\"nr_sample_targets\"]\n",
    "#     )\n",
    "# else:\n",
    "#     sample_targets = targ_route_not_in_route_dict_NEW\n",
    "# # Create targ_routes_dict_sample with the sampled keys and their corresponding values\n",
    "# targ_route_not_in_route_dict_sample = {\n",
    "#     target: targ_route_not_in_route_dict[target] for target in sample_targets\n",
    "# }\n",
    "\n",
    "input_data_NEW = targ_route_not_in_route_dict_NEW\n",
    "\n",
    "if config[\"model_type\"] == \"gnn\":\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "\n",
    "    purch_mols = [Chem.MolFromSmiles(smiles) for smiles in purch_smiles]\n",
    "    purch_featurizer = featurizer.featurize(purch_mols)\n",
    "    purch_featurizer_dict = dict(zip(purch_smiles, purch_featurizer))\n",
    "\n",
    "    dataset_NEW = gnn_preprocess_input(\n",
    "        input_data=input_data_NEW, \n",
    "        featurizer=featurizer, \n",
    "        featurizer_dict=purch_featurizer_dict,\n",
    "        pos_sampling=config[\"pos_sampling\"],\n",
    "    )\n",
    "\n",
    "elif config[\"model_type\"] == \"fingerprints\":\n",
    "    purch_fingerprints = list(map(fingerprint_vect_from_smiles, purch_smiles))\n",
    "    purch_fingerprints_dict = dict(zip(purch_smiles, purch_fingerprints))\n",
    "\n",
    "    dataset_NEW = fingerprint_preprocess_input(\n",
    "        input_data_NEW, \n",
    "        fingerprints_dict=purch_fingerprints_dict, \n",
    "        pos_sampling=config[\"pos_sampling\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "\n",
    "# Train validation split\n",
    "validation_ratio = config[\"validation_ratio\"]\n",
    "num_samples_NEW = len(dataset_NEW)\n",
    "num_val_samples_NEW = int(validation_ratio * num_samples_NEW)\n",
    "\n",
    "train_indices_NEW, val_indices_NEW = train_test_split(\n",
    "    range(num_samples_NEW), test_size=num_val_samples_NEW, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset_NEW = Subset(dataset_NEW, train_indices_NEW)\n",
    "val_dataset_NEW = Subset(dataset_NEW, val_indices_NEW)\n",
    "\n",
    "train_data_loader_NEW = DataLoader(\n",
    "    train_dataset_NEW,\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    shuffle=config[\"train_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_data_loader_NEW = DataLoader(\n",
    "    val_dataset_NEW,\n",
    "    batch_size=config[\"val_batch_size\"],\n",
    "    shuffle=config[\"val_shuffle\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cccc2ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88190617bfc642969e0f2121a89e9f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  3.594730608165264\n"
     ]
    }
   ],
   "source": [
    "val_loss_NEW = 0.0\n",
    "val_batches_NEW = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for val_batch_idx, val_batch_data in tqdm(enumerate(val_data_loader_NEW)):\n",
    "        # Compute embeddings\n",
    "        val_embeddings_NEW = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model,\n",
    "            batch_data=val_batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        val_batch_loss_NEW = loss_fn_loaded(val_embeddings_NEW)\n",
    "\n",
    "        val_loss_NEW += val_batch_loss_NEW.item()\n",
    "        val_batches_NEW += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    average_val_loss_NEW = val_loss_NEW / val_batches_NEW\n",
    "\n",
    "print(\"Validation loss: \", average_val_loss_NEW)\n",
    "\n",
    "# If it high --> Problem with sampling of the negatives\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04a996f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b052071a0e47158c0f4fb7da7ab1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  3.5931588113307953\n"
     ]
    }
   ],
   "source": [
    "val_loss_NEW = 0.0\n",
    "val_batches_NEW = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for val_batch_idx, val_batch_data in tqdm(enumerate(val_data_loader_NEW)):\n",
    "        # Compute embeddings\n",
    "        val_embeddings_NEW = compute_embeddings(\n",
    "            device=device,\n",
    "            model_type=config['model_type'],\n",
    "            model=model_loaded,\n",
    "            batch_data=val_batch_data,\n",
    "        )\n",
    "        # Compute loss\n",
    "        val_batch_loss_NEW = loss_fn_loaded(val_embeddings_NEW)\n",
    "\n",
    "        val_loss_NEW += val_batch_loss_NEW.item()\n",
    "        val_batches_NEW += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    average_val_loss_NEW = val_loss_NEW / val_batches_NEW\n",
    "\n",
    "print(\"Validation loss: \", average_val_loss_NEW)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a25022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0182,  0.0882,  0.0097,  ..., -0.0630,  0.0137, -0.0142],\n",
       "        [-0.0137, -0.0323,  0.0690,  ...,  0.0586, -0.0718,  0.0425],\n",
       "        [-0.0948, -0.0169, -0.2129,  ..., -0.0453,  0.0261,  0.0008],\n",
       "        ...,\n",
       "        [ 0.0455,  0.0312,  0.0367,  ...,  0.1213,  0.0331, -0.1026],\n",
       "        [ 0.0481,  0.1346,  0.0286,  ..., -0.0846,  0.0547, -0.0316],\n",
       "        [ 0.0465, -0.1177,  0.0113,  ...,  0.0605,  0.0737,  0.0774]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[\"conv1.lin.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef1c6e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0182,  0.0882,  0.0097,  ..., -0.0630,  0.0137, -0.0142],\n",
       "        [-0.0137, -0.0323,  0.0690,  ...,  0.0586, -0.0718,  0.0425],\n",
       "        [-0.0948, -0.0169, -0.2129,  ..., -0.0453,  0.0261,  0.0008],\n",
       "        ...,\n",
       "        [ 0.0455,  0.0312,  0.0367,  ...,  0.1213,  0.0331, -0.1026],\n",
       "        [ 0.0481,  0.1346,  0.0286,  ..., -0.0846,  0.0547, -0.0316],\n",
       "        [ 0.0465, -0.1177,  0.0113,  ...,  0.0605,  0.0737,  0.0774]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.state_dict()[\"conv1.lin.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7bffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
