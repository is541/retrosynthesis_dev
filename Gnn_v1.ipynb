{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210180cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Jax models, missing a dependency. jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import deepchem as dc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "# from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e21916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json.config\n",
    "with open('config_gnn.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c62b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_in_route_sample_size': 100, 'seed': 42, 'run_id': '202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7', 'nr_sample_targets': -1, 'model_type': 'gnn', 'batch_size': 64, 'shuffle': True, 'hidden_dim': 512, 'output_dim': 256, 'temperature': 0.1, 'lr': 0.001, 'num_epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7b34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"{config['model_type']}_v1\"\n",
    "\n",
    "checkpoint_folder = f'GraphRuns/{experiment_name}/'\n",
    "# if not os.path.exists(checkpoint_folder):\n",
    "#     os.makedirs(checkpoint_folder)\n",
    "\n",
    "checkpoint_name = 'checkpoint.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393680c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gnn_v1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991c98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "with open(f'{checkpoint_folder}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972dc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preprocessed_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cd352",
   "metadata": {},
   "source": [
    "### Read routes data - consider only route 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c34f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_routes = f'Runs/{config[\"run_id\"] }/targ_routes.pickle'\n",
    "input_file_distances = f'Runs/{config[\"run_id\"]}/targ_to_purch_distances.pickle'\n",
    "\n",
    "# Routes data\n",
    "with open(input_file_routes, 'rb') as handle:\n",
    "    targ_routes_dict = pickle.load(handle)\n",
    "    \n",
    "# Load distances data\n",
    "with open(input_file_distances, 'rb') as handle:\n",
    "    distances_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Inventory\n",
    "from paroutes import PaRoutesInventory\n",
    "inventory=PaRoutesInventory(n=5)\n",
    "purch_smiles = [mol.smiles for mol in inventory.purchasable_mols()]\n",
    "len(purch_smiles)\n",
    "\n",
    "def num_heavy_atoms(mol):\n",
    "    return Chem.rdchem.Mol.GetNumAtoms(mol, onlyExplicit=True)\n",
    "\n",
    "purch_mol_to_exclude = []\n",
    "for smiles in purch_smiles:\n",
    "    if num_heavy_atoms(Chem.MolFromSmiles(smiles)) < 2:\n",
    "        purch_mol_to_exclude = purch_mol_to_exclude + [smiles]\n",
    "\n",
    "\n",
    "\n",
    "targ_route_not_in_route_dict = {}\n",
    "for target, target_routes_dict in targ_routes_dict.items():\n",
    "    targ_route_not_in_route_dict[target] = {}\n",
    "    \n",
    "    target_route_df = target_routes_dict[\"route_1\"]\n",
    "    purch_in_route = list(target_route_df['smiles'])\n",
    "    purch_not_in_route = [purch_smile for purch_smile in purch_smiles if purch_smile not in purch_in_route]\n",
    "    random.seed(config[\"seed\"])\n",
    "    purch_not_in_route_sample = random.sample(purch_not_in_route, config[\"not_in_route_sample_size\"])\n",
    "    \n",
    "    # Filter out molecules with only one atom (problems with featurizer)\n",
    "    purch_in_route = [smiles for smiles in purch_in_route if smiles not in purch_mol_to_exclude]\n",
    "    purch_not_in_route_sample = [smiles for smiles in purch_not_in_route_sample if smiles not in purch_mol_to_exclude]\n",
    "    \n",
    "    targ_route_not_in_route_dict[target]['positive_samples'] = purch_in_route\n",
    "    targ_route_not_in_route_dict[target]['negative_samples'] = purch_not_in_route_sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49573e",
   "metadata": {},
   "source": [
    "### Temp - select a sample of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e572c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of keys from targ_routes_dict\n",
    "if config[\"nr_sample_targets\"]!= -1:\n",
    "    sample_targets = random.sample(list(targ_route_not_in_route_dict.keys()), config[\"nr_sample_targets\"])\n",
    "else:\n",
    "    sample_targets = targ_route_not_in_route_dict\n",
    "\n",
    "\n",
    "# Create targ_routes_dict_sample with the sampled keys and their corresponding values\n",
    "targ_route_not_in_route_dict_sample = {target: targ_route_not_in_route_dict[target] for target in sample_targets}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da030ef2",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7b650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_preprocess_input(input_data):\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "    targets = []\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         try:\n",
    "        target_feats = featurizer.featurize(Chem.MolFromSmiles(target_smiles))\n",
    "        pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "        neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "        pos_feats = featurizer.featurize(pos_mols)\n",
    "        neg_feats = featurizer.featurize(neg_mols)\n",
    "\n",
    "        targets.append(target_feats[0])\n",
    "        positive_samples.append(pos_feats)\n",
    "        negative_samples.append(neg_feats)\n",
    "#             targets_torch = torch.tensor(target_feats, dtype=torch.double)\n",
    "#             positive_samples = torch.tensor(pos_feats, dtype=torch.double)\n",
    "#             negative_samples = torch.tensor(neg_feats, dtype=torch.double)\n",
    "#             targets.append(targets_torch)\n",
    "#             positive_samples.append(positive_samples)\n",
    "#             negative_samples.append(negative_samples)\n",
    "            \n",
    "#         except:\n",
    "#             # Handle the case where featurization fails for a sample\n",
    "#             print(f\"Featurization failed for sample: {target_smiles}\")\n",
    "        \n",
    "#     targets_tensor = torch.stack(targets)\n",
    "#     positive_samples_tensor = torch.stack(positive_samples)\n",
    "#     negative_samples_tensor = torch.stack(negative_samples)\n",
    "#     return targets_tensor, positive_samples_tensor, negative_samples_tensor\n",
    "    return targets, positive_samples, negative_samples\n",
    "\n",
    "\n",
    "def fingerprint_vect_from_smiles(mol_smiles):\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(AllChem.MolFromSmiles(mol_smiles), radius=3)\n",
    "\n",
    "def fingerprint_preprocess_input(input_data):\n",
    "    targets = []\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         target_feats = fingerprint_from_smiles(Chem.MolFromSmiles(target_smiles))\n",
    "#         pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "#         neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "        target_feats = fingerprint_vect_from_smiles(target_smiles)\n",
    "        pos_feats = list(map(fingerprint_vect_from_smiles, samples['positive_samples']))\n",
    "        neg_feats = list(map(fingerprint_vect_from_smiles, samples['negative_samples']))\n",
    "        \n",
    "#         targets.append(target_feats[0])\n",
    "#         positive_samples.append(pos_feats)\n",
    "#         negative_samples.append(neg_feats)\n",
    "        targets.append(torch.tensor(target_feats, dtype=torch.double))\n",
    "        positive_samples.append(torch.tensor(pos_feats, dtype=torch.double))\n",
    "        negative_samples.append(torch.tensor(neg_feats, dtype=torch.double))\n",
    "        \n",
    "\n",
    "    return targets, positive_samples, negative_samples\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, targets, positive_samples, negative_samples):\n",
    "        self.targets = targets\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.targets[idx]\n",
    "        positive = self.positive_samples[idx]\n",
    "        negative = self.negative_samples[idx]\n",
    "\n",
    "        return target, positive, negative\n",
    "\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     print(\"Using collate_fn\")\n",
    "#     targets = []\n",
    "#     positive_samples = []\n",
    "#     negative_samples = []\n",
    "\n",
    "#     for target, positive, negative in data:\n",
    "#         targets.append(target)\n",
    "#         positive_samples.extend(positive)\n",
    "#         negative_samples.extend(negative)\n",
    "\n",
    "#     targets = torch.stack(targets, dim=0)\n",
    "#     positive_samples = torch.stack(positive_samples, dim=0)\n",
    "#     negative_samples = torch.stack(negative_samples, dim=0)\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "\n",
    "def collate_fn(data):\n",
    "    targets, positive_samples, negative_samples = zip(*data)\n",
    "\n",
    "    return targets, positive_samples, negative_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "189777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleData:\n",
    "    def __init__(self, target, positive_samples, negative_samples):\n",
    "        self.target = target\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "\n",
    "# def preprocess_input(input_data):\n",
    "#     featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "#     data_list = []\n",
    "    \n",
    "#     for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         target_feats = featurizer.featurize(Chem.MolFromSmiles(target_smiles))\n",
    "#         pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "#         neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "#         pos_feats = featurizer.featurize(pos_mols)\n",
    "#         neg_feats = featurizer.featurize(neg_mols)\n",
    "#         data_list = data_list + [SampleData(target=target_feats, positive_samples=pos_feats, negative_samples=neg_feats)]\n",
    "#     return data_list\n",
    "        \n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.data[idx]\n",
    "# #         return sample\n",
    "\n",
    "# #         # Extract the target_smiles, positive_sample, and negative_samples\n",
    "# # #         target_smiles = sample['target_smiles']\n",
    "# # #         positive_sample = sample['positive_sample']\n",
    "# # #         negative_samples = sample['negative_samples']\n",
    "#         target = sample.target\n",
    "#         positive_samples = sample.positive_samples\n",
    "#         negative_samples = sample.negative_samples\n",
    "\n",
    "#         # Convert the data to tensors or any other necessary preprocessing\n",
    "\n",
    "#         # Return the sample with named attributes\n",
    "# #         return {\n",
    "# #             'target': target,\n",
    "# #             'positive_samples': positive_samples,\n",
    "# #             'negative_samples': negative_samples\n",
    "# #         }\n",
    "#         return target, positive_samples, negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a799c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define embedding model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def custom_global_max_pool(x):\n",
    "    return torch.max(x, dim=0)[0]\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#     def __init__(self, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "#         self.conv1 = GCNConv(None,hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global max pooling (from node level to graph level embeddings)\n",
    "#         x = global_max_pool(x) #, edge_index[0]\n",
    "        x = custom_global_max_pool(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FingerprintModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FingerprintModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step 3: Create a contrastive learning loss function\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        sample_losses = []\n",
    "        for single_sample_embeddings in embeddings:\n",
    "            target_emb = single_sample_embeddings.target\n",
    "            positive_embs = single_sample_embeddings.positive_samples\n",
    "            negative_embs = single_sample_embeddings.negative_samples\n",
    "            \n",
    "            # Sample one positive\n",
    "            nr_positives = positive_embs.size(0)\n",
    "            # Randomly select a positive sample\n",
    "            row_index = torch.randint(0, nr_positives, (1,))\n",
    "            positive_emb = torch.index_select(positive_embs, dim=0, index=row_index)\n",
    "#             positive_emb = positive_emb.squeeze(0)\n",
    "            \n",
    "            positive_similarity = self.cos_sim(target_emb, positive_emb)\n",
    "            positive_similarity /= self.temperature           \n",
    "            negative_similarity = self.cos_sim(target_emb, negative_embs)\n",
    "            negative_similarity /= self.temperature\n",
    "            \n",
    "            numerator = torch.exp(positive_similarity)\n",
    "            denominator = torch.sum(torch.exp(negative_similarity))\n",
    "            sample_loss = -torch.log(numerator / (numerator + denominator))\n",
    "            sample_losses = sample_losses + [sample_loss]\n",
    "        \n",
    "        return sum(sample_losses) / len(sample_losses)\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from pytorch_metric_learning.losses import GenericPairLoss\n",
    "\n",
    "# class NTXentLoss(GenericPairLoss):\n",
    "\n",
    "#     def __init__(self, temperature, **kwargs):\n",
    "#         super().__init__(use_similarity=True, mat_based_loss=False, **kwargs)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def _compute_loss(self, pos_pairs, neg_pairs, indices_tuple):\n",
    "#         a1, p, a2, _ = indices_tuple\n",
    "\n",
    "#         if len(a1) > 0 and len(a2) > 0:\n",
    "#             pos_pairs = pos_pairs.unsqueeze(1) / self.temperature\n",
    "#             neg_pairs = neg_pairs / self.temperature\n",
    "#             n_per_p = (a2.unsqueeze(0) == a1.unsqueeze(1)).float()\n",
    "#             neg_pairs = neg_pairs*n_per_p\n",
    "#             neg_pairs[n_per_p==0] = float('-inf')\n",
    "\n",
    "#             max_val = torch.max(pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0].half()) ###This is the line change\n",
    "#             numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "#             denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1) + numerator\n",
    "#             log_exp = torch.log((numerator/denominator) + 1e-20)\n",
    "#             return {\"loss\": {\"losses\": -log_exp, \"indices\": (a1, p), \"reduction_type\": \"pos_pair\"}}\n",
    "#         return self.zero_losses()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Or use NTXentMultiplePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2fa61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 9895/9895 [1:13:36<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Prepare the dataset\n",
    "# # Assuming we have a dataset file named 'molecules.csv' with molecular structures and labels\n",
    "# dataset = dc.data.CSVLoader(tasks=['property'], feature_field='smiles')\n",
    "# dataset.load_from_file('molecules.csv')\n",
    "# splitter = dc.splits.RandomSplitter()\n",
    "# train_dataset, valid_dataset, _ = splitter.train_valid_test_split(dataset)\n",
    "\n",
    "# Step 1: Create data dictionary getting negative samples for each target\n",
    "input_data = targ_route_not_in_route_dict_sample\n",
    "\n",
    "\n",
    "# Step 2: Featurizer and cast as CustomDataset\n",
    "# Step 3: Create DataLoader\n",
    "\n",
    "\n",
    "if config[\"model_type\"] == 'gnn':\n",
    "    preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples = gnn_preprocess_input(input_data)\n",
    "    dataset = CustomDataset(preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples)\n",
    "    data_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=config[\"shuffle\"], collate_fn=collate_fn)\n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "    preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples = fingerprint_preprocess_input(input_data)\n",
    "    dataset = CustomDataset(preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples)\n",
    "    data_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=config[\"shuffle\"], collate_fn=collate_fn)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "    \n",
    "    \n",
    "\n",
    "# def collate_fn(data):\n",
    "#     targets = [sample[0] for sample in data]\n",
    "#     positive_samples = [sample[1] for sample in data]\n",
    "#     negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "# def collate_fn(data):\n",
    "#     targets = [sample[0] for sample in data]\n",
    "#     positive_samples = [sample[1] for sample in data]\n",
    "#     negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "\n",
    "# Save\n",
    "if save_preprocessed_data:\n",
    "    with open(f'{checkpoint_folder}/preprocessed_targets.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_targets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'{checkpoint_folder}/preprocessed_positive_samples.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_positive_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'{checkpoint_folder}/preprocessed_negative_samples.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_negative_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Load\n",
    "# with open(f'{checkpoint_folder}/preprocessed_targets.pickle', 'wb') as handle:\n",
    "#     preprocessed_targets = pickle.load(handle)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_positive_samples.pickle', 'wb') as handle:\n",
    "#     preprocessed_positive_samples = pickle.load(handle)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_negative_samples.pickle', 'wb') as handle:\n",
    "#     preprocessed_negative_samples = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04bf86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function collate_fn at 0x2d45fe440>\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd6eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(preprocessed_targets[0])\n",
    "# (preprocessed_targets[0].size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a497a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model_type\"] == 'gnn':\n",
    "    gnn_input_dim = preprocessed_targets[0].node_features.shape[1]\n",
    "    gnn_hidden_dim = config[\"hidden_dim\"]\n",
    "    gnn_output_dim = config[\"output_dim\"]\n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "#     fingerprint_input_dim = preprocessed_targets[0].GetNumBits()\n",
    "    fingerprint_input_dim = len(preprocessed_targets[0].node_features) #(preprocessed_targets[0].size()[0])\n",
    "    fingerprint_hidden_dim = config[\"hidden_dim\"]\n",
    "    fingerprint_output_dim = config[\"output_dim\"]\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3165193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a6f88a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2d58d5de0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c66b9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up the training loop for the GNN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if config[\"model_type\"] == 'gnn':\n",
    "    model = GNNModel(\n",
    "        input_dim=gnn_input_dim, \n",
    "        hidden_dim=gnn_hidden_dim, \n",
    "        output_dim=gnn_output_dim).to(device)\n",
    "    model.double()\n",
    "    \n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "    model = FingerprintModel(\n",
    "        input_dim=fingerprint_input_dim, \n",
    "        hidden_dim=fingerprint_hidden_dim, \n",
    "        output_dim=fingerprint_output_dim).to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = NTXentLoss(temperature=config[\"temperature\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ddda7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_gnn_embedding(gnn_model):\n",
    "#     node_features = torch.tensor(example.node_features, dtype=torch.double)\n",
    "#     edge_index = torch.tensor(example.edge_index, dtype=torch.long)  # Assuming edge_index is of type 'long'\n",
    "\n",
    "#     # Convert the input node features to double\n",
    "#     node_features = node_features.double()\n",
    "\n",
    "#     # Compute the embeddings for the positive example\n",
    "#     embedding = model(node_features, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b84332da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.collate_fn(data)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca7f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653842f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                       | 1/100 [24:21<40:10:50, 1461.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn Model - Epoch 1/100, Loss: 2.103701420761078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███████▌                                                             | 11/100 [4:47:43<39:34:54, 1601.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn Model - Epoch 11/100, Loss: 0.18274515600282853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████▉                                                            | 13/100 [5:40:24<38:31:01, 1593.82s/it]"
     ]
    }
   ],
   "source": [
    "# Check if a checkpoint exists and load the model state and optimizer state if available\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create a SummaryWriter for TensorBoard logging\n",
    "log_dir = f'{checkpoint_folder}/logs'  # Specify the directory to store TensorBoard logs\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "model.train()\n",
    "epoch_loss = pd.DataFrame(columns=['Epoch', 'TrainLoss'])\n",
    "for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for batch_idx, batch_data in enumerate(train_data_loader):\n",
    "        batch_targets, batch_positive_samples, batch_negative_samples = batch_data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = []\n",
    "        for i in range(len(batch_targets)):\n",
    "            target = batch_targets[i]\n",
    "            positives = batch_positive_samples[i]\n",
    "            negatives = batch_negative_samples[i]\n",
    "            if config[\"model_type\"] == 'gnn':\n",
    "                target_node_features = torch.tensor(target.node_features, dtype=torch.double)\n",
    "                target_edge_index = torch.tensor(target.edge_index, dtype=torch.long)  # Assuming edge_index is of type 'long'\n",
    "                target_embedding = model(target_node_features, target_edge_index)\n",
    "                \n",
    "                positive_samples_embeddings = torch.stack([\n",
    "                    model(torch.tensor(example.node_features, dtype=torch.double).double(),\n",
    "                          torch.tensor(example.edge_index, dtype=torch.long)) \n",
    "                    for example in positives\n",
    "                ], dim=0)\n",
    "                \n",
    "                negative_samples_embeddings = torch.stack([\n",
    "                    model(torch.tensor(example.node_features, dtype=torch.double).double(),\n",
    "                          torch.tensor(example.edge_index, dtype=torch.long)) \n",
    "                    for example in negatives\n",
    "                ], dim=0)\n",
    "                embeddings = embeddings + [SampleData(target=target_embedding, positive_samples=positive_samples_embeddings, negative_samples=negative_samples_embeddings)]\n",
    "\n",
    "            elif config[\"model_type\"] == 'fingerprints':\n",
    "                target_embedding = model(target)\n",
    "                positive_samples_embeddings = model(positives)\n",
    "                negative_samples_embeddings = model(negatives)\n",
    "                embeddings = embeddings + [SampleData(target=target_embedding, positive_samples=positive_samples_embeddings, negative_samples=negative_samples_embeddings)]\n",
    "            else:\n",
    "                raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(embeddings)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total loss\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    average_loss = total_loss / total_batches\n",
    "    \n",
    "    new_row = pd.DataFrame({'Epoch': [epoch], 'TrainLoss': [average_loss]})\n",
    "    epoch_loss = pd.concat([epoch_loss, new_row], axis=0)\n",
    "    \n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch+1)\n",
    "\n",
    "\n",
    "    \n",
    "    if ((epoch%10==0) | (epoch==num_epochs-1)):\n",
    "        print(f\"{config['model_type']} Model - Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Save the model and optimizer state as a checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint_path = f'{checkpoint_folder}/epoch_{epoch+1}_{checkpoint_name}'  # Specify the checkpoint file path\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "#         loss_df = pd.DataFrame({'Epoch': range(len(epoch_loss)), 'TrainLoss': epoch_loss})\n",
    "        epoch_loss.to_csv(f'{checkpoint_folder}/train_loss.csv', index=False)\n",
    "        \n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Step 4: Evaluate and use the trained embeddings\n",
    "# You can evaluate the embeddings on downstream tasks or use them for molecular similarity search or property prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e736cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(x=epoch_loss['Epoch'], y=epoch_loss['TrainLoss'], title=\"Train loss\")\n",
    "fig.update_layout(width=1000, height=600, showlegend=False)\n",
    "fig.write_image(f\"{checkpoint_folder}/Train_loss.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e57b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save\n",
    "# with open(f'{checkpoint_folder}/preprocessed_targets.pickle', 'wb') as handle:\n",
    "#         pickle.dump(preprocessed_targets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_positive_samples.pickle', 'wb') as handle:\n",
    "#         pickle.dump(preprocessed_positive_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_negative_samples.pickle', 'wb') as handle:\n",
    "#         pickle.dump(preprocessed_negative_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0b8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
