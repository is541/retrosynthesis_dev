{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210180cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import deepchem as dc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "# from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e21916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json.config\n",
    "with open('config_fnp_nn_0630.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c62b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_name': 'gnn_0629', 'not_in_route_sample_size': 100, 'seed': 42, 'run_id': '202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7', 'nr_sample_targets': -1, 'model_type': 'gnn', 'validation_ratio': 0.2, 'train_batch_size': 64, 'train_shuffle': True, 'val_batch_size': 64, 'val_shuffle': False, 'pos_sampling': 'uniform', 'neg_sampling': 'uniform', 'hidden_dim': 512, 'output_dim': 256, 'temperature': 0.1, 'lr': 0.001, 'num_epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7b34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"{config['experiment_name']}\"\n",
    "\n",
    "checkpoint_folder = f'GraphRuns/{experiment_name}/'\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "checkpoint_name = 'checkpoint.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393680c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gnn_0629'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991c98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "with open(f'{checkpoint_folder}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972dc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preprocessed_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cd352",
   "metadata": {},
   "source": [
    "### Read routes data - consider only route 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c34f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_routes = f'Runs/{config[\"run_id\"]}/targ_routes.pickle'\n",
    "# input_file_distances = f'Runs/{config[\"run_id\"]}/targ_to_purch_distances.pickle'\n",
    "\n",
    "# Routes data\n",
    "with open(input_file_routes, 'rb') as handle:\n",
    "    targ_routes_dict = pickle.load(handle)\n",
    "    \n",
    "# # Load distances data\n",
    "# with open(input_file_distances, 'rb') as handle:\n",
    "#     distances_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Inventory\n",
    "from paroutes import PaRoutesInventory, get_target_smiles\n",
    "\n",
    "inventory=PaRoutesInventory(n=5)\n",
    "purch_smiles = [mol.smiles for mol in inventory.purchasable_mols()]\n",
    "len(purch_smiles)\n",
    "\n",
    "def num_heavy_atoms(mol):\n",
    "    return Chem.rdchem.Mol.GetNumAtoms(mol, onlyExplicit=True)\n",
    "\n",
    "purch_mol_to_exclude = []\n",
    "purch_nr_heavy_atoms = {}\n",
    "for smiles in purch_smiles:\n",
    "    nr_heavy_atoms = num_heavy_atoms(Chem.MolFromSmiles(smiles))\n",
    "    if nr_heavy_atoms < 2:\n",
    "        purch_mol_to_exclude = purch_mol_to_exclude + [smiles]\n",
    "    purch_nr_heavy_atoms[smiles] = nr_heavy_atoms \n",
    "\n",
    "    \n",
    "if config[\"run_id\"]==\"202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7\":\n",
    "    all_targets = get_target_smiles(n=5)\n",
    "elif config[\"run_id\"]=='Guacamol_combined':\n",
    "     with open('Data/Guacamol/guacamol_v1_test_10ksample.txt', \"r\") as f:\n",
    "        all_targets = [line.strip() for line in f.readlines()]  \n",
    "\n",
    "    \n",
    "targ_route_not_in_route_dict = {}\n",
    "for target in all_targets:\n",
    "    \n",
    "    targ_route_not_in_route_dict[target] = {}\n",
    "    \n",
    "    target_routes_dict = targ_routes_dict.get(target, 'Target_Not_Solved')\n",
    "    \n",
    "    if target_routes_dict=='Target_Not_Solved':\n",
    "        purch_in_route = []\n",
    "    else:\n",
    "        target_route_df = target_routes_dict[\"route_1\"]\n",
    "        purch_in_route = list(target_route_df.loc[target_route_df['label']!='Target', 'smiles'])\n",
    "#         purch_in_route = [smiles for smiles in purch_in_route if smiles in purch_smiles]\n",
    "    purch_not_in_route = [purch_smile for purch_smile in purch_smiles if purch_smile not in purch_in_route]\n",
    "    random.seed(config[\"seed\"])\n",
    "    \n",
    "    if config[\"neg_sampling\"] == \"uniform\":\n",
    "        purch_not_in_route_sample = random.sample(purch_not_in_route, config[\"not_in_route_sample_size\"])\n",
    "    elif config[\"neg_sampling\"] == \"...\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f'{config[\"neg_sampling\"]}')\n",
    "    \n",
    "    # Filter out molecules with only one atom (problems with featurizer)\n",
    "    purch_in_route = [smiles for smiles in purch_in_route if smiles not in purch_mol_to_exclude]\n",
    "    purch_not_in_route_sample = [smiles for smiles in purch_not_in_route_sample if smiles not in purch_mol_to_exclude]\n",
    "    \n",
    "    targ_route_not_in_route_dict[target]['positive_samples'] = purch_in_route\n",
    "    targ_route_not_in_route_dict[target]['negative_samples'] = purch_not_in_route_sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de0fc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Br', 'I', 'Cl', '[S-2]', 'N', 'S', 'F', 'O', '[Mg]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purch_mol_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d263de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_samples': ['O=[N+]([O-])O',\n",
       "  'CC(=O)Cl',\n",
       "  'CCc1ccccc1OC',\n",
       "  'O=P(Br)(Br)Br',\n",
       "  'O=NO'],\n",
       " 'negative_samples': ['COC(=O)c1ccc(Br)c(C)c1',\n",
       "  'CC(=O)Nc1ccc(B(O)O)cc1',\n",
       "  'C[C@H](Cc1c[nH]c2ccccc12)NCC(C)(C)F',\n",
       "  'NC(=O)Nc1ccc(S(=O)(=O)Cl)cc1',\n",
       "  'COC(=O)c1ccc(-c2cc(OC)ncc2F)c(C2(C=O)CCCC2)c1',\n",
       "  'CC(C)(C)[SiH2]OC(C)(C)c1cccc(CO)n1',\n",
       "  'CCCC(C)(C#N)NC(=O)OC(C)(C)C',\n",
       "  'C[C@H]1NC(=O)c2cc(B3OC(C)(C)C(C)(C)O3)[nH]c21',\n",
       "  'CC(C)(C)OC(=O)Nc1ccc(F)c([C@]2(C)Cn3c(nc(Cl)c3C#N)C(N)=N2)c1',\n",
       "  'COC(=O)C(CO)(CO)c1cc(-c2nc3cc(C(=N)N)ccc3[nH]2)c(O)c(-c2cc(C#N)ccc2O)c1',\n",
       "  'O=C1c2ccccc2C(=O)N1[C@@H]1CCC[C@@H]1c1ccccc1',\n",
       "  'C[C@@H](c1cc2cccc(Cl)c2nc1C(=O)O)N1C(=O)c2ccccc2C1=O',\n",
       "  'C#CC',\n",
       "  'OCC(CO)CO',\n",
       "  'O=C(CBr)c1cccc(Cl)c1',\n",
       "  'Cc1nn2c(Cl)cc(Cl)nc2c1Cc1cccc(C(F)(F)F)c1C',\n",
       "  'COc1ccc(Oc2ccccc2)c(N)c1',\n",
       "  'COC(=O)c1ccc2nc(C)c(-c3ccccc3)n2c1',\n",
       "  'Brc1cnc2c(c1)OCCN2',\n",
       "  'COC(=O)C=Cc1ccc(CO)cc1C',\n",
       "  'OCc1cc(Cl)c(Cl)c(Cl)c1',\n",
       "  'COC(=O)[C@@H]1C[C@H](NC2CCC(C)(C)CC2)CN1C(=O)OC(C)(C)C',\n",
       "  'Nc1ccc(SC(F)(F)F)cc1',\n",
       "  'O=C(O)/C=C/c1cnc2[nH]c(=O)oc2c1',\n",
       "  'CCOC(=O)C(C)(CCCc1ccccc1)Cc1ccc(OCc2ccccc2)cc1',\n",
       "  'C=CCc1c(O)ccc(C(=O)OC)c1O',\n",
       "  'O=c1[nH]cnc2c([N+](=O)[O-])cccc12',\n",
       "  'CC1(C)OC[C@H](COc2ccc(-c3c(C#N)c(N)nc(S)c3C#N)cc2)O1',\n",
       "  'COC(=O)Cc1ccc2oc(Cc3cccnc3C)cc2c1',\n",
       "  'CC(=O)c1ccc(O)cc1O',\n",
       "  'C=Cc1c(NC(C(=O)O)C(C)O)ccc(C#N)c1Cl',\n",
       "  'CCOC(=O)c1nn(-c2ccccc2)c(N)c1C',\n",
       "  'COc1cccc(C)c1C',\n",
       "  'CCOC(=O)CC(C[N+](=O)[O-])C1(NC(=O)OC(C)(C)C)CC(OCc2ccccc2)C1',\n",
       "  'NC1(c2ccc(F)c(F)c2)CC1',\n",
       "  'CNCCNC(=O)c1nc(Cl)c(N)nc1N',\n",
       "  'O=C1CCN(C(=O)c2ccc(F)cc2)CC1',\n",
       "  'CCC/C=C/C(=O)O',\n",
       "  'Nc1cccc(N)n1',\n",
       "  'Clc1cc(N2CCOCC2)nc(Cl)n1',\n",
       "  'CCOC(=O)C1CCN(C(=O)OC(C)(C)C)C1c1cc(C)nc(-n2ccnc2)n1',\n",
       "  'Cc1ccc(N)c(N)c1',\n",
       "  'CC(=O)Oc1cc2c(cc1C(C)(C)CC(C)(C)C)OC(C)(COc1ccc([N+](=O)[O-])cc1)CC2=O',\n",
       "  'CSCCO',\n",
       "  'CCOC(=O)c1c(N)sc2c1CN(C(=O)OCC)C2',\n",
       "  'Cc1cccc(-c2ncccn2)c1',\n",
       "  'COC(=O)c1cccc(I)c1C(=O)OC',\n",
       "  'CCOC(=O)c1cc(Cl)ccn1',\n",
       "  'COc1cc(C(=O)O)ccc1N',\n",
       "  'CC(O)CN',\n",
       "  'Fc1ccc(OC[C@@H]2CO2)cc1',\n",
       "  'O=C(n1cncn1)n1cncn1',\n",
       "  'COC(=O)c1cc(C(C)=O)c(C2CCC2)cc1C',\n",
       "  'Ic1ccnc2c1CCO2',\n",
       "  'COc1ccc2nc(Cl)cc(CC#N)c2c1',\n",
       "  'N#CCc1ccc(B(O)O)cc1',\n",
       "  'Fc1ccccc1I',\n",
       "  'N#Cc1ccc(CCl)cc1',\n",
       "  'CCCN1CCc2ccc(C(=O)OC)cc2CC1',\n",
       "  'O=C(O)C1CCCc2cc(OCc3ccccc3)ccc21',\n",
       "  'Fc1cccc(CBr)c1',\n",
       "  'FC(F)(F)Sc1ccc(Cl)nc1',\n",
       "  'BrCCOCCBr',\n",
       "  'O=Cc1ccc(Br)cc1F',\n",
       "  'Cc1cc(C)cc(N)c1',\n",
       "  'Clc1ccnc2[nH]ccc12',\n",
       "  'Nc1cc(Cl)c(O)c(Cl)c1',\n",
       "  'Oc1ccccc1-c1cc[nH]n1',\n",
       "  'NS(=O)(=O)N1CCC2(CC1)OCCO2',\n",
       "  'C[Si](C)(C)c1ccc([N+](=O)[O-])c(F)c1',\n",
       "  'O=C(O)Cc1cc(F)cc(F)c1',\n",
       "  'OB(O)c1cc2ccccc2s1',\n",
       "  'Cc1ncc(Cl)cc1C(=O)Cl',\n",
       "  'O=C(Cl)Cc1ccccc1',\n",
       "  'O=Cc1ccc[nH]1',\n",
       "  'FC(F)(F)c1cccc(N2CCNCC2)c1',\n",
       "  'O=C(O)c1ccc(-c2cnco2)nc1',\n",
       "  'O=C(O)CCC(=O)c1cc([N+](=O)[O-])ccc1O',\n",
       "  'Cn1cc(Br)c2ccccc2c1=O',\n",
       "  'CC(C)(C)OC(=O)NC1CCN(C2CCOCC2)CC1',\n",
       "  'CC(C)(O)c1ccc2c(c1)[nH]c1c(C(N)=O)ccc(Br)c12',\n",
       "  'COC(=O)c1cccc(Br)c1',\n",
       "  'COc1cc(N)cc(-c2cccc(F)c2)c1',\n",
       "  'Cc1ccc(N)c(C)c1',\n",
       "  'CCOC(=O)Cc1cc(=O)oc2cc(O)ccc12',\n",
       "  'COC(=O)CCCCC(=O)O',\n",
       "  'Cc1ccc(Cl)nc1',\n",
       "  'Nc1ccc(I)cc1',\n",
       "  'O=S(=O)(c1ccccc1)c1c(Cl)[nH]c2ccc(Cl)cc12',\n",
       "  'CC1(C)OB(c2ccc(C3(C(=O)NS(C)(=O)=O)CC3)cc2)OC1(C)C',\n",
       "  'CCOC(=O)C1CCCC1=O',\n",
       "  'Cc1cccnc1CN',\n",
       "  'CCOC(=O)c1c(CBr)cccc1[N+](=O)[O-]',\n",
       "  'CNc1ccc([N+](=O)[O-])cc1',\n",
       "  'CC1(O)CCCCC1',\n",
       "  'O=C(O)c1cc[nH]c1',\n",
       "  'Cc1ccccc1',\n",
       "  'Nc1ccc(C(=O)O)c2c1OCCO2',\n",
       "  'COc1ccc(CNc2nc(SC)ncc2C(=O)O)cc1Cl',\n",
       "  'NCC(O)CN1Cc2ccccc2C1']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targ_route_not_in_route_dict['COc1ccc(F)c(-c2ccc(COc3cc(C(CC(=O)O)C4CC4)ccc3F)nc2CC(C)(C)C)c1'] \n",
    "# No positives\n",
    "\n",
    "# targ_route_not_in_route_dict['CCc1cc2c(Br)cnnc2cc1OC']\n",
    "# Si positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38abc888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCc1cc2c(Br)cnnc2cc1OC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(targ_route_not_in_route_dict.keys())[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3bb4a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9895"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targ_routes_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20e86ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targ_route_not_in_route_dict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49573e",
   "metadata": {},
   "source": [
    "### Temp - select a sample of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e572c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of keys from targ_routes_dict\n",
    "if config[\"nr_sample_targets\"]!= -1:\n",
    "    sample_targets = random.sample(list(targ_route_not_in_route_dict.keys()), config[\"nr_sample_targets\"])\n",
    "else:\n",
    "    sample_targets = targ_route_not_in_route_dict\n",
    "\n",
    "\n",
    "# Create targ_routes_dict_sample with the sampled keys and their corresponding values\n",
    "targ_route_not_in_route_dict_sample = {target: targ_route_not_in_route_dict[target] for target in sample_targets}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da030ef2",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be7b650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_preprocess_input(input_data, featurizer, purch_featurizer_dict):\n",
    "#     featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "    targets = []\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         try:\n",
    "        target_feats = featurizer.featurize(Chem.MolFromSmiles(target_smiles))\n",
    "        pos_feats = [purch_featurizer_dict[positive_smiles] for positive_smiles in samples['positive_samples']]\n",
    "        neg_feats = [purch_featurizer_dict[negative_smiles] for negative_smiles in samples['negative_samples']]\n",
    "#         pos_feats = featurizer.featurize(pos_mols)\n",
    "#         neg_feats = featurizer.featurize(neg_mols)\n",
    "\n",
    "        targets.append(target_feats[0])\n",
    "        positive_samples.append(pos_feats)\n",
    "        negative_samples.append(neg_feats)\n",
    "#             targets_torch = torch.tensor(target_feats, dtype=torch.double)\n",
    "#             positive_samples = torch.tensor(pos_feats, dtype=torch.double)\n",
    "#             negative_samples = torch.tensor(neg_feats, dtype=torch.double)\n",
    "#             targets.append(targets_torch)\n",
    "#             positive_samples.append(positive_samples)\n",
    "#             negative_samples.append(negative_samples)\n",
    "            \n",
    "#         except:\n",
    "#             # Handle the case where featurization fails for a sample\n",
    "#             print(f\"Featurization failed for sample: {target_smiles}\")\n",
    "        \n",
    "#     targets_tensor = torch.stack(targets)\n",
    "#     positive_samples_tensor = torch.stack(positive_samples)\n",
    "#     negative_samples_tensor = torch.stack(negative_samples)\n",
    "#     return targets_tensor, positive_samples_tensor, negative_samples_tensor\n",
    "    return targets, positive_samples, negative_samples\n",
    "\n",
    "\n",
    "def fingerprint_vect_from_smiles(mol_smiles):\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(AllChem.MolFromSmiles(mol_smiles), radius=3)\n",
    "\n",
    "def fingerprint_preprocess_input(input_data, purch_fingerprints_dict):\n",
    "    targets = []\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         target_feats = fingerprint_from_smiles(Chem.MolFromSmiles(target_smiles))\n",
    "#         pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "#         neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "        target_feats = fingerprint_vect_from_smiles(target_smiles)\n",
    "#         pos_feats = list(map(fingerprint_vect_from_smiles, samples['positive_samples']))\n",
    "#         neg_feats = list(map(fingerprint_vect_from_smiles, samples['negative_samples']))\n",
    "        pos_feats = [purch_fingerprints_dict[positive_smiles] for positive_smiles in samples['positive_samples']]\n",
    "        neg_feats = [purch_fingerprints_dict[negative_smiles] for negative_smiles in samples['negative_samples']]\n",
    "        \n",
    "#         targets.append(target_feats[0])\n",
    "#         positive_samples.append(pos_feats)\n",
    "#         negative_samples.append(neg_feats)\n",
    "        targets.append(torch.tensor(target_feats, dtype=torch.double))\n",
    "        positive_samples.append(torch.tensor(pos_feats, dtype=torch.double))\n",
    "        negative_samples.append(torch.tensor(neg_feats, dtype=torch.double))\n",
    "        \n",
    "\n",
    "    return targets, positive_samples, negative_samples\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, targets, positive_samples, negative_samples):\n",
    "        self.targets = targets\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.targets[idx]\n",
    "        positive = self.positive_samples[idx]\n",
    "        negative = self.negative_samples[idx]\n",
    "\n",
    "        return target, positive, negative\n",
    "\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     print(\"Using collate_fn\")\n",
    "#     targets = []\n",
    "#     positive_samples = []\n",
    "#     negative_samples = []\n",
    "\n",
    "#     for target, positive, negative in data:\n",
    "#         targets.append(target)\n",
    "#         positive_samples.extend(positive)\n",
    "#         negative_samples.extend(negative)\n",
    "\n",
    "#     targets = torch.stack(targets, dim=0)\n",
    "#     positive_samples = torch.stack(positive_samples, dim=0)\n",
    "#     negative_samples = torch.stack(negative_samples, dim=0)\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "\n",
    "def collate_fn(data):\n",
    "    targets, positive_samples, negative_samples = zip(*data)\n",
    "\n",
    "    return targets, positive_samples, negative_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "189777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleData:\n",
    "    def __init__(self, target, positive_samples, negative_samples, pos_weights):\n",
    "        self.target = target\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "        self.pos_weights = pos_weights\n",
    "\n",
    "\n",
    "# def preprocess_input(input_data):\n",
    "#     featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "#     data_list = []\n",
    "    \n",
    "#     for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         target_feats = featurizer.featurize(Chem.MolFromSmiles(target_smiles))\n",
    "#         pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "#         neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "#         pos_feats = featurizer.featurize(pos_mols)\n",
    "#         neg_feats = featurizer.featurize(neg_mols)\n",
    "#         data_list = data_list + [SampleData(target=target_feats, positive_samples=pos_feats, negative_samples=neg_feats)]\n",
    "#     return data_list\n",
    "        \n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.data[idx]\n",
    "# #         return sample\n",
    "\n",
    "# #         # Extract the target_smiles, positive_sample, and negative_samples\n",
    "# # #         target_smiles = sample['target_smiles']\n",
    "# # #         positive_sample = sample['positive_sample']\n",
    "# # #         negative_samples = sample['negative_samples']\n",
    "#         target = sample.target\n",
    "#         positive_samples = sample.positive_samples\n",
    "#         negative_samples = sample.negative_samples\n",
    "\n",
    "#         # Convert the data to tensors or any other necessary preprocessing\n",
    "\n",
    "#         # Return the sample with named attributes\n",
    "# #         return {\n",
    "# #             'target': target,\n",
    "# #             'positive_samples': positive_samples,\n",
    "# #             'negative_samples': negative_samples\n",
    "# #         }\n",
    "#         return target, positive_samples, negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07a799c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define embedding model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def custom_global_max_pool(x):\n",
    "    return torch.max(x, dim=0)[0]\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#     def __init__(self, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global max pooling (from node level to graph level embeddings)\n",
    "#         x = global_max_pool(x) #, edge_index[0]\n",
    "        x = custom_global_max_pool(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FingerprintModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FingerprintModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# from scipy.special import logsumexp\n",
    "\n",
    "# Step 3: Create a contrastive learning loss function\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        sample_losses = []\n",
    "        for single_sample_embeddings in embeddings:\n",
    "            target_emb = single_sample_embeddings.target\n",
    "            positive_embs = single_sample_embeddings.positive_samples\n",
    "            negative_embs = single_sample_embeddings.negative_samples\n",
    "            pos_weights = single_sample_embeddings.pos_weights\n",
    "            \n",
    "            \n",
    "            # Positive similarity\n",
    "            nr_positives = positive_embs.size(0)\n",
    "            if nr_positives == 0:\n",
    "                positive_similarity = torch.tensor(0.0)\n",
    "            else:\n",
    "                # Sample one positive\n",
    "                if pos_weights is None:\n",
    "                    # Randomly select a positive sample\n",
    "                    row_index = torch.randint(0, nr_positives, (1,))\n",
    "                    positive_emb = torch.index_select(positive_embs, dim=0, index=row_index)\n",
    "\n",
    "                else:\n",
    "                    assert len(pos_weights) == nr_positives, f'len pos_weight {len(pos_weights)} different from nr_positives{nr_positives} '            \n",
    "                    row_index = torch.multinomial(pos_weights, 1)\n",
    "                    positive_emb = torch.index_select(positive_embs, dim=0, index=row_index)\n",
    "            \n",
    "                positive_similarity = self.cos_sim(target_emb, positive_emb)\n",
    "                positive_similarity /= self.temperature           \n",
    "            \n",
    "            # Negative similarity\n",
    "            negative_similarity = self.cos_sim(target_emb, negative_embs)\n",
    "            negative_similarity /= self.temperature\n",
    "            \n",
    "            # Old implementation\n",
    "            numerator = torch.exp(positive_similarity)\n",
    "            denominator = torch.sum(torch.exp(negative_similarity))\n",
    "            sample_loss = -torch.log(numerator / (numerator + denominator))\n",
    "            # End Old implementation\n",
    "#             # New implementation\n",
    "#             all_similarities = torch.cat([positive_similarity, negative_similarity], dim=0)\n",
    "#             sample_loss = -positive_similarity + torch.logsumexp(all_similarities, dim=0, keepdims=True)\n",
    "#             # End New implementation\n",
    "            \n",
    "            sample_losses = sample_losses + [sample_loss]\n",
    "        \n",
    "        return sum(sample_losses) / len(sample_losses)\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from pytorch_metric_learning.losses import GenericPairLoss\n",
    "\n",
    "# class NTXentLoss(GenericPairLoss):\n",
    "\n",
    "#     def __init__(self, temperature, **kwargs):\n",
    "#         super().__init__(use_similarity=True, mat_based_loss=False, **kwargs)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def _compute_loss(self, pos_pairs, neg_pairs, indices_tuple):\n",
    "#         a1, p, a2, _ = indices_tuple\n",
    "\n",
    "#         if len(a1) > 0 and len(a2) > 0:\n",
    "#             pos_pairs = pos_pairs.unsqueeze(1) / self.temperature\n",
    "#             neg_pairs = neg_pairs / self.temperature\n",
    "#             n_per_p = (a2.unsqueeze(0) == a1.unsqueeze(1)).float()\n",
    "#             neg_pairs = neg_pairs*n_per_p\n",
    "#             neg_pairs[n_per_p==0] = float('-inf')\n",
    "\n",
    "#             max_val = torch.max(pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0].half()) ###This is the line change\n",
    "#             numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "#             denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1) + numerator\n",
    "#             log_exp = torch.log((numerator/denominator) + 1e-20)\n",
    "#             return {\"loss\": {\"losses\": -log_exp, \"indices\": (a1, p), \"reduction_type\": \"pos_pair\"}}\n",
    "#         return self.zero_losses()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Or use NTXentMultiplePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a2fa61a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m featurizer \u001b[38;5;241m=\u001b[39m dc\u001b[38;5;241m.\u001b[39mfeat\u001b[38;5;241m.\u001b[39mMolGraphConvFeaturizer()\n\u001b[1;32m     19\u001b[0m purch_mols \u001b[38;5;241m=\u001b[39m [Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(smiles) \u001b[38;5;28;01mfor\u001b[39;00m smiles \u001b[38;5;129;01min\u001b[39;00m purch_smiles]\n\u001b[0;32m---> 20\u001b[0m purch_featurizer \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpurch_mols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m purch_featurizer_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(purch_smiles, purch_featurizer))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/purch_featurizer_dict.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/deepchem/feat/base_classes.py:313\u001b[0m, in \u001b[0;36mMolecularFeaturizer.featurize\u001b[0;34m(self, datapoints, log_every_n, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    312\u001b[0m     kwargs_per_datapoint[key] \u001b[38;5;241m=\u001b[39m kwargs[key][i]\n\u001b[0;32m--> 313\u001b[0m   features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_featurize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_per_datapoint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mol, Chem\u001b[38;5;241m.\u001b[39mrdchem\u001b[38;5;241m.\u001b[39mMol):\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/deepchem/feat/molecule_featurizers/mol_graph_conv_featurizer.py:207\u001b[0m, in \u001b[0;36mMolGraphConvFeaturizer._featurize\u001b[0;34m(self, datapoint, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# construct atom (node) feature\u001b[39;00m\n\u001b[1;32m    205\u001b[0m h_bond_infos \u001b[38;5;241m=\u001b[39m construct_hydrogen_bonding_info(datapoint)\n\u001b[1;32m    206\u001b[0m atom_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m--> 207\u001b[0m     [\n\u001b[1;32m    208\u001b[0m         _construct_atom_feature(atom, h_bond_infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_chirality,\n\u001b[1;32m    209\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_partial_charge)\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m atom \u001b[38;5;129;01min\u001b[39;00m datapoint\u001b[38;5;241m.\u001b[39mGetAtoms()\n\u001b[1;32m    211\u001b[0m     ],\n\u001b[1;32m    212\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# construct edge (bond) index\u001b[39;00m\n\u001b[1;32m    216\u001b[0m src, dest \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Step 1: Prepare the dataset\n",
    "# # Assuming we have a dataset file named 'molecules.csv' with molecular structures and labels\n",
    "# dataset = dc.data.CSVLoader(tasks=['property'], feature_field='smiles')\n",
    "# dataset.load_from_file('molecules.csv')\n",
    "# splitter = dc.splits.RandomSplitter()\n",
    "# train_dataset, valid_dataset, _ = splitter.train_valid_test_split(dataset)\n",
    "\n",
    "# Step 1: Create data dictionary getting negative samples for each target\n",
    "input_data = targ_route_not_in_route_dict_sample\n",
    "\n",
    "\n",
    "# Step 2: Featurizer and cast as CustomDataset\n",
    "# Step 3: Create DataLoader\n",
    "\n",
    "\n",
    "if config[\"model_type\"] == 'gnn':\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "    \n",
    "    purch_mols = [Chem.MolFromSmiles(smiles) for smiles in purch_smiles]\n",
    "    purch_featurizer = featurizer.featurize(purch_mols)\n",
    "    purch_featurizer_dict = dict(zip(purch_smiles, purch_featurizer))\n",
    "    with open(f'{checkpoint_folder}/purch_featurizer_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(purch_featurizer_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    fingerprint_num_atoms_dict = None\n",
    "    \n",
    "    preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples = gnn_preprocess_input(input_data, featurizer, purch_featurizer_dict)\n",
    "    dataset = CustomDataset(preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples)\n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "    purch_fingerprints = list(map(fingerprint_vect_from_smiles, purch_smiles))\n",
    "    purch_fingerprints_dict = dict(zip(purch_smiles, purch_fingerprints))\n",
    "    with open(f'{checkpoint_folder}/purch_fingerprints_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(purch_fingerprints_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Also save dict to retrieve number of atoms from fingerprints\n",
    "    fingerprint_num_atoms_dict = {fp: purch_nr_heavy_atoms[smiles] for smiles, fp in purch_fingerprints_dict.items()}\n",
    "    with open(f'{checkpoint_folder}/fingerprint_num_atoms_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(fingerprint_num_atoms_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples = fingerprint_preprocess_input(input_data, purch_fingerprints_dict)\n",
    "    dataset = CustomDataset(preprocessed_targets, preprocessed_positive_samples, preprocessed_negative_samples)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "    \n",
    "    \n",
    "\n",
    "# def collate_fn(data):\n",
    "#     targets = [sample[0] for sample in data]\n",
    "#     positive_samples = [sample[1] for sample in data]\n",
    "#     negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "# def collate_fn(data):\n",
    "#     targets = [sample[0] for sample in data]\n",
    "#     positive_samples = [sample[1] for sample in data]\n",
    "#     negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "\n",
    "# Save\n",
    "if save_preprocessed_data:\n",
    "    with open(f'{checkpoint_folder}/preprocessed_targets.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_targets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'{checkpoint_folder}/preprocessed_positive_samples.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_positive_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'{checkpoint_folder}/preprocessed_negative_samples.pickle', 'wb') as handle:\n",
    "            pickle.dump(preprocessed_negative_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Load\n",
    "# with open(f'{checkpoint_folder}/preprocessed_targets.pickle', 'wb') as handle:\n",
    "#     preprocessed_targets = pickle.load(handle)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_positive_samples.pickle', 'wb') as handle:\n",
    "#     preprocessed_positive_samples = pickle.load(handle)\n",
    "# with open(f'{checkpoint_folder}/preprocessed_negative_samples.pickle', 'wb') as handle:\n",
    "#     preprocessed_negative_samples = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67384521",
   "metadata": {},
   "source": [
    "##### Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = config[\"validation_ratio\"]\n",
    "num_samples = len(dataset)\n",
    "num_val_samples = int(validation_ratio * num_samples)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(num_samples), test_size=num_val_samples, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=config[\"train_batch_size\"], shuffle=config[\"train_shuffle\"], collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=config[\"val_batch_size\"], shuffle=config[\"val_shuffle\"], collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Batch size: The batch size determines the number of samples processed in each iteration during training or validation. In most cases, it is common to use the same batch size for both training and validation to maintain consistency. However, there are situations where you might choose a different batch size for validation. For instance, if memory constraints are more relaxed during validation, you can use a larger batch size to speed up evaluation.\n",
    "# Shuffle training data: Shuffling the training data before each epoch is beneficial because it helps the model see the data in different orders, reducing the risk of the model learning patterns specific to the order of the data. Shuffling the training data introduces randomness and promotes better generalization.\n",
    "# No shuffle for validation data: It is generally not necessary to shuffle the validation data because validation is meant to evaluate the model's performance on unseen data that is representative of the real-world scenarios. Shuffling the validation data could lead to inconsistent evaluation results between different validation iterations, making it harder to track the model's progress and compare performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04bf86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function collate_fn at 0x2cc63bc70>\n"
     ]
    }
   ],
   "source": [
    "print(train_data_loader.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd6eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(preprocessed_targets[0])\n",
    "# (preprocessed_targets[0].size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529c2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef2f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"model_type\"] == 'gnn':\n",
    "    gnn_input_dim = preprocessed_targets[0].node_features.shape[1]\n",
    "    gnn_hidden_dim = config[\"hidden_dim\"]\n",
    "    gnn_output_dim = config[\"output_dim\"]\n",
    "    \n",
    "    with open(f'{checkpoint_folder}/input_dim.pickle' , 'wb') as f:\n",
    "        pickle.dump({'input_dim': gnn_input_dim}, f)\n",
    "    \n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "#     fingerprint_input_dim = preprocessed_targets[0].GetNumBits()\n",
    "    fingerprint_input_dim = (preprocessed_targets[0].size()[0]) # len(preprocessed_targets[0].node_features)\n",
    "    fingerprint_hidden_dim = config[\"hidden_dim\"]\n",
    "    fingerprint_output_dim = config[\"output_dim\"]\n",
    "    \n",
    "    with open(f'{checkpoint_folder}/input_dim.pickle' , 'wb') as f:\n",
    "        pickle.dump({'input_dim': fingerprint_input_dim}, f)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c66b9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up the training loop for the GNN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if config[\"model_type\"] == 'gnn':\n",
    "    model = GNNModel(\n",
    "        input_dim=gnn_input_dim, \n",
    "        hidden_dim=gnn_hidden_dim, \n",
    "        output_dim=gnn_output_dim).to(device)\n",
    "    model.double()\n",
    "    \n",
    "elif config[\"model_type\"] == 'fingerprints':\n",
    "    model = FingerprintModel(\n",
    "        input_dim=fingerprint_input_dim, \n",
    "        hidden_dim=fingerprint_hidden_dim, \n",
    "        output_dim=fingerprint_output_dim).to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = NTXentLoss(temperature=config[\"temperature\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ddda7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_gnn_embedding(gnn_model):\n",
    "#     node_features = torch.tensor(example.node_features, dtype=torch.double)\n",
    "#     edge_index = torch.tensor(example.edge_index, dtype=torch.long)  # Assuming edge_index is of type 'long'\n",
    "\n",
    "#     # Convert the input node features to double\n",
    "#     node_features = node_features.double()\n",
    "\n",
    "#     # Compute the embeddings for the positive example\n",
    "#     embedding = model(node_features, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b84332da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.collate_fn(data)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ca7f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1c585c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_and_loss(model, batch_targets, batch_positive_samples, batch_negative_samples, loss_fn, pos_sampling, fingerprint_num_atoms_dict=None):\n",
    "    embeddings = []\n",
    "    for i in range(len(batch_targets)):\n",
    "        target = batch_targets[i]\n",
    "        positives = batch_positive_samples[i]\n",
    "        negatives = batch_negative_samples[i]\n",
    "\n",
    "        if config[\"model_type\"] == 'gnn':\n",
    "            target_node_features = torch.tensor(target.node_features, dtype=torch.double)\n",
    "            target_edge_index = torch.tensor(target.edge_index, dtype=torch.long)\n",
    "            target_embedding = model(target_node_features, target_edge_index)\n",
    "            \n",
    "            if len(positives)==0:\n",
    "                positive_samples_embeddings = torch.empty((0, target_embedding.size(0)))\n",
    "            else:\n",
    "                positive_samples_embeddings = torch.stack([\n",
    "                    model(torch.tensor(example.node_features, dtype=torch.double),\n",
    "                          torch.tensor(example.edge_index, dtype=torch.long))\n",
    "                    for example in positives\n",
    "                ], dim=0)\n",
    "\n",
    "            negative_samples_embeddings = torch.stack([\n",
    "                model(torch.tensor(example.node_features, dtype=torch.double),\n",
    "                      torch.tensor(example.edge_index, dtype=torch.long))\n",
    "                for example in negatives\n",
    "            ], dim=0)\n",
    "\n",
    "            \n",
    "            if pos_sampling == \"uniform\":\n",
    "                pos_weights = None\n",
    "            elif pos_sampling == \"prop_num_atoms\":\n",
    "                pos_weights = []\n",
    "                for positive in positives:\n",
    "                    pos_weights.append(positive.node_features.shape[0])\n",
    "#                 print('Len pos weights', len(pos_weights))\n",
    "                pos_weights = torch.tensor(pos_weights, dtype=torch.double)\n",
    "                # Normalize the tensor to sum up to 1\n",
    "                pos_weights = pos_weights / pos_weights.sum()\n",
    "            else:\n",
    "                raise NotImplementedError(f'{config[\"pos_sampling\"]}')\n",
    "                \n",
    "            embeddings.append(\n",
    "                SampleData(target=target_embedding, positive_samples=positive_samples_embeddings,\n",
    "                           negative_samples=negative_samples_embeddings, pos_weights=pos_weights)\n",
    "            )\n",
    "\n",
    "\n",
    "        elif config[\"model_type\"] == 'fingerprints':\n",
    "            target_embedding = model(target)\n",
    "            if len(positives)==0:\n",
    "                positive_samples_embeddings = torch.empty((0, target_embedding.size(0)))\n",
    "            else:\n",
    "                positive_samples_embeddings = model(positives)\n",
    "            negative_samples_embeddings = model(negatives)\n",
    "\n",
    "            \n",
    "            if pos_sampling == \"uniform\":\n",
    "                pos_weights = None\n",
    "            elif pos_sampling == \"prop_num_atoms\":\n",
    "                pos_weights = [fingerprint_num_atoms_dict[positive]for positive in positives]  \n",
    "                pos_weights = torch.tensor(pos_weights, dtype=torch.double)\n",
    "                # Normalize the tensor to sum up to 1\n",
    "                pos_weights = pos_weights / pos_weights.sum()\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f'{config[\"pos_sampling\"]}')\n",
    "            embeddings.append(\n",
    "                SampleData(target=target_embedding, positive_samples=positive_samples_embeddings,\n",
    "                           negative_samples=negative_samples_embeddings, pos_weights=pos_weights)\n",
    "            )\n",
    "            \n",
    "#             embeddings = embeddings + [SampleData(target=target_embedding, positive_samples=positive_samples_embeddings, negative_samples=negative_samples_embeddings)]\n",
    "        else:\n",
    "            raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "    # Compute loss for the batch\n",
    "    loss = loss_fn(embeddings)\n",
    "\n",
    "    return embeddings, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "653842f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                            | 1/100 [22:30<37:07:46, 1350.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn Model - Epoch 1/100, TrainLoss: 3.3300414522612325, ValLoss: 2.778004522677617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                            | 1/100 [41:13<68:01:57, 2473.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m batch_targets, batch_positive_samples, batch_negative_samples \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m embeddings, loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_embeddings_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_positive_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_negative_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos_sampling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfingerprint_num_atoms_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[74], line 25\u001b[0m, in \u001b[0;36mcompute_embeddings_and_loss\u001b[0;34m(model, batch_targets, batch_positive_samples, batch_negative_samples, loss_fn, pos_sampling, fingerprint_num_atoms_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     positive_samples_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     20\u001b[0m         model(torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39mnode_features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble),\n\u001b[1;32m     21\u001b[0m               torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39medge_index, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m positives\n\u001b[1;32m     23\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m negative_samples_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     26\u001b[0m     model(torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39mnode_features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble),\n\u001b[1;32m     27\u001b[0m           torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39medge_index, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m negatives\n\u001b[1;32m     29\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_sampling \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     33\u001b[0m     pos_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     positive_samples_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m     20\u001b[0m         model(torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39mnode_features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble),\n\u001b[1;32m     21\u001b[0m               torch\u001b[38;5;241m.\u001b[39mtensor(example\u001b[38;5;241m.\u001b[39medge_index, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m positives\n\u001b[1;32m     23\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m negative_samples_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m negatives\n\u001b[1;32m     29\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_sampling \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     33\u001b[0m     pos_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[67], line 27\u001b[0m, in \u001b[0;36mGNNModel.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     25\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index)\n\u001b[1;32m     26\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 27\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# Global max pooling (from node level to graph level embeddings)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#         x = global_max_pool(x) #, edge_index[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:182\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if a checkpoint exists and load the model state and optimizer state if available\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create a SummaryWriter for TensorBoard logging\n",
    "log_dir = f'{checkpoint_folder}/logs'  # Specify the directory to store TensorBoard logs\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "epoch_loss = pd.DataFrame(columns=['Epoch', 'TrainLoss', 'ValLoss'])\n",
    "for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(train_data_loader):\n",
    "        batch_targets, batch_positive_samples, batch_negative_samples = batch_data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings, loss = compute_embeddings_and_loss(model, batch_targets, batch_positive_samples, batch_negative_samples, loss_fn, config[\"pos_sampling\"], fingerprint_num_atoms_dict)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total loss\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for val_batch_idx, val_batch_data in enumerate(val_data_loader):\n",
    "            val_batch_targets, val_batch_positive_samples, val_batch_negative_samples = val_batch_data\n",
    "\n",
    "            val_embeddings, val_batch_loss = compute_embeddings_and_loss(model, val_batch_targets,\n",
    "                                                                     val_batch_positive_samples,\n",
    "                                                                     val_batch_negative_samples, loss_fn, config[\"pos_sampling\"], fingerprint_num_atoms_dict)\n",
    "\n",
    "\n",
    "            val_loss += val_batch_loss.item()\n",
    "            val_batches += 1\n",
    "            \n",
    "    # METRICS\n",
    "    # - TRAIN\n",
    "    # Compute average loss for the epoch\n",
    "    average_train_loss = train_loss / train_batches\n",
    "        \n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar('Loss/train', average_train_loss, epoch+1)\n",
    "    \n",
    "    # - VALIDATION\n",
    "    average_val_loss = val_loss / val_batches\n",
    "    \n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar('Loss/val', average_val_loss, epoch+1)\n",
    "    \n",
    "    new_row = pd.DataFrame({'Epoch': [epoch], 'TrainLoss': [average_train_loss], 'ValLoss': [average_val_loss]})\n",
    "    epoch_loss = pd.concat([epoch_loss, new_row], axis=0)\n",
    "\n",
    "    \n",
    "    if ((epoch%10==0) | (epoch==num_epochs-1)):\n",
    "        print(f\"{config['model_type']} Model - Epoch {epoch+1}/{num_epochs}, TrainLoss: {average_train_loss}, ValLoss: {average_val_loss}\")\n",
    "        \n",
    "        # Save the model and optimizer state as a checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint_path = f'{checkpoint_folder}/epoch_{epoch+1}_{checkpoint_name}'  # Specify the checkpoint file path\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "#         loss_df = pd.DataFrame({'Epoch': range(len(epoch_loss)), 'TrainLoss': epoch_loss})\n",
    "        epoch_loss.to_csv(f'{checkpoint_folder}/train_val_loss.csv', index=False)\n",
    "    \n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_model = model\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Save the best model as a pickle\n",
    "best_model_path = f'{checkpoint_folder}/model_min_val.pkl' #'path/to/best_model.pkl'\n",
    "\n",
    "with open(best_model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e736cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# fig = px.line(x=epoch_loss['Epoch'], y=epoch_loss['TrainLoss'], title=\"Train loss\")\n",
    "# fig.update_layout(width=1000, height=600, showlegend=False)\n",
    "# fig.write_image(f\"{checkpoint_folder}/Train_loss.pdf\")\n",
    "# fig.show()\n",
    "\n",
    "# Create a new figure with two lines\n",
    "fig = px.line()\n",
    "\n",
    "# Add the TrainLoss line to the figure\n",
    "fig.add_scatter(x=epoch_loss['Epoch'], y=epoch_loss['TrainLoss'], name='Train Loss')\n",
    "\n",
    "# Add the ValLoss line to the figure\n",
    "fig.add_scatter(x=epoch_loss['Epoch'], y=epoch_loss['ValLoss'], name='Validation Loss')\n",
    "\n",
    "# Set the title of the figure\n",
    "fig.update_layout(title=\"Train and Validation Loss\")\n",
    "\n",
    "# Set the layout size and show the legend\n",
    "fig.update_layout(width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "fig.write_image(f\"{checkpoint_folder}/Train_and_Val_loss.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96f0b8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % np.nan == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3820c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
