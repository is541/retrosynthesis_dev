{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3179f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=/Users/ilariasartori/syntheseus:/Users/ilariasartori/syntheseus/tutorials/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0859e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b11cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202306-2611-1616-0f701fe7-bbee-4d2c-83f7-bba18beb858a\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "# eventid = datetime.now().strftime('%Y%m-%d%H-%M%S-') + str(uuid4())\n",
    "eventid = '202306-2611-1616-0f701fe7-bbee-4d2c-83f7-bba18beb858a'\n",
    "print(eventid)\n",
    "\n",
    "output_folder = f\"CompareTanimotoLearnt/{eventid}\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d9e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96d1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic code for nearest-neighbour value functions.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "from rdkit.Chem import DataStructs, AllChem\n",
    "\n",
    "from syntheseus.search.graph.and_or import OrNode\n",
    "from syntheseus.search.node_evaluation.base import NoCacheNodeEvaluator\n",
    "from syntheseus.search.mol_inventory import ExplicitMolInventory\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from Users.ilariasartori.syntheseus.search.graph.and_or import OrNode\n",
    "\n",
    "\n",
    "class DistanceToCost(Enum):\n",
    "    NOTHING = 0\n",
    "    EXP = 1\n",
    "    SQRT = 2\n",
    "    TIMES10 = 3\n",
    "    TIMES100 = 4\n",
    "    NUM_NEIGHBORS_TO_1 = 5\n",
    "\n",
    "\n",
    "class TanimotoNNCostEstimator(NoCacheNodeEvaluator):\n",
    "    \"\"\"Estimates cost of a node using Tanimoto distance to purchasable molecules.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inventory: ExplicitMolInventory,\n",
    "        distance_to_cost: DistanceToCost,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.distance_to_cost = distance_to_cost\n",
    "        self._set_fingerprints([mol.smiles for mol in inventory.purchasable_mols()])\n",
    "\n",
    "    def get_fingerprint(self, mol: AllChem.Mol):\n",
    "        return AllChem.GetMorganFingerprint(mol, radius=3)\n",
    "\n",
    "    def _set_fingerprints(self, smiles_list: list[str]) -> None:\n",
    "        \"\"\"Initialize fingerprint cache.\"\"\"\n",
    "        mols = list(map(AllChem.MolFromSmiles, smiles_list))\n",
    "        assert None not in mols, \"Invalid SMILES encountered.\"\n",
    "        self._fps = list(map(self.get_fingerprint, mols))\n",
    "        \n",
    "    def find_min_num_elem_summing_to_threshold(self, array, threshold):\n",
    "        # Sort the array in ascending order\n",
    "        sorted_array = np.sort(array)[::-1]\n",
    "\n",
    "        # Calculate the cumulative sum of the sorted array\n",
    "        cum_sum = np.cumsum(sorted_array)\n",
    "\n",
    "        # Find the index where the cumulative sum exceeds threshold \n",
    "        index = np.searchsorted(cum_sum, threshold)\n",
    "\n",
    "        # Check if a subset of elements sums up to more than threshold\n",
    "        if index < len(array):\n",
    "            return index + 1  # Add 1 to account for 0-based indexing\n",
    "\n",
    "        # If no subset of elements sums up to more than threshold\n",
    "        return len(array) #-1\n",
    "\n",
    "    def _get_nearest_neighbour_dist(self, smiles: str) -> float:\n",
    "        fp_query = self.get_fingerprint(AllChem.MolFromSmiles(smiles))\n",
    "        tanimoto_sims = DataStructs.BulkTanimotoSimilarity(fp_query, self._fps)\n",
    "        if self.distance_to_cost == DistanceToCost.NUM_NEIGHBORS_TO_1:\n",
    "            return 1 - self.find_min_num_elem_summing_to_threshold(array=tanimoto_sims,threshold=1)/ len(tanimoto_sims)\n",
    "        else:\n",
    "            return 1 - max(tanimoto_sims)\n",
    "\n",
    "    def _evaluate_nodes(self, nodes: list[OrNode], graph=None) -> list[float]:\n",
    "        if len(nodes) == 0:\n",
    "            return []\n",
    "\n",
    "        # Get distances to nearest neighbours\n",
    "        nn_dists = np.asarray(\n",
    "            [self._get_nearest_neighbour_dist(node.mol.smiles) for node in nodes]\n",
    "        )\n",
    "        assert np.min(nn_dists) >= 0, f'Negative distance: {np.min(nn_dists)} '\n",
    "\n",
    "        # Turn into costs\n",
    "        if self.distance_to_cost == DistanceToCost.NOTHING:\n",
    "            values = nn_dists\n",
    "        elif self.distance_to_cost == DistanceToCost.EXP:\n",
    "            values = np.exp(nn_dists) - 1\n",
    "        elif self.distance_to_cost == DistanceToCost.SQRT:\n",
    "            values = np.sqrt(nn_dists) \n",
    "        elif self.distance_to_cost == DistanceToCost.TIMES10:\n",
    "            values = 10.0*nn_dists\n",
    "        elif self.distance_to_cost == DistanceToCost.TIMES100:\n",
    "            values = 100.0*nn_dists\n",
    "        elif self.distance_to_cost == DistanceToCost.NUM_NEIGHBORS_TO_1:\n",
    "            values = nn_dists\n",
    "        else:\n",
    "            raise NotImplementedError(self.distance_to_cost)\n",
    "\n",
    "        return list(values)\n",
    "\n",
    "\n",
    "class Emb_from_fingerprints_NNCostEstimator(NoCacheNodeEvaluator):\n",
    "    \"\"\"Estimates cost of a node using Tanimoto distance to purchasable molecules.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inventory: ExplicitMolInventory,\n",
    "        distance_to_cost: DistanceToCost_emb_fnps,\n",
    "        model,\n",
    "        distance_type,\n",
    "        **kwargs,\n",
    "    ):\n",
    "#         print('Stat initialization Emb')\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.distance_to_cost = distance_to_cost\n",
    "        self.distance_type = distance_type\n",
    "        self._set_fingerprints_vect([mol.smiles for mol in inventory.purchasable_mols()])\n",
    "        with torch.no_grad():\n",
    "            self.emb_purch_molecules = torch.stack([self.model(torch.tensor(fingerprint, dtype=torch.double)) for fingerprint in self._fps], dim=0)\n",
    "#         print('End initialization Emb')\n",
    "\n",
    "    def get_fingerprint_vect(self, mol: AllChem.Mol):\n",
    "        return AllChem.GetMorganFingerprintAsBitVect(mol, radius=3)\n",
    "\n",
    "    def _set_fingerprints_vect(self, smiles_list: list[str]) -> None:\n",
    "        \"\"\"Initialize fingerprint cache.\"\"\"\n",
    "        mols = list(map(AllChem.MolFromSmiles, smiles_list))\n",
    "        assert None not in mols, \"Invalid SMILES encountered.\"\n",
    "        self._fps = list(map(self.get_fingerprint_vect, mols))\n",
    "        \n",
    "\n",
    "    def compute_embedding_from_fingerprint(self, mol_fingerprints):\n",
    "#         self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(torch.tensor(mol_fingerprints, dtype=torch.double))\n",
    "#             if isinstance(mol_fingerprints, list):\n",
    "#                 output = \n",
    "#             else:\n",
    "                \n",
    "        return output\n",
    "\n",
    "    def embedding_distance(self, emb_1, emb_2):\n",
    "        if self.distance_type == 'Euclidean':\n",
    "            # Compute Euclidean distance\n",
    "            euclidean_distance = torch.norm(emb_1 - emb_2, dim=1)\n",
    "            return euclidean_distance\n",
    "        elif self.distance_type == 'cosine':\n",
    "            # Compute cosine similarity\n",
    "            cosine_similarity = F.cosine_similarity(emb_1, emb_2, dim=1)\n",
    "#             print(cosine_similarity)\n",
    "#             cosine_distance = 1 - cosine_similarity\n",
    "            cosine_distance = torch.clamp(1 - cosine_similarity, min=0, max=1)\n",
    "#             print(cosine_distance)\n",
    "            \n",
    "#             if np.min(cosine_distance)< 0 or np.max(cosine_distance) > 1:\n",
    "#                 if abs(cosine_distance - 0) < 1e-10:\n",
    "#                     return 0\n",
    "#                 elif abs(cosine_distance - 1) < 1e-10:\n",
    "#                     return 1\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Cosine distance not between 0 and 1: Min: {np.min(cosine_distance)}, Max:{np.max(cosine_distance)}\")            \n",
    "            return cosine_distance\n",
    "        else:\n",
    "            # Raise error for unsupported distance type\n",
    "            raise NotImplementedError(f\"Distance type '{self.distance_type}' is not implemented.\")\n",
    "\n",
    "    def _get_nearest_neighbour_dist(self, smiles: str) -> float:\n",
    "        fp_target = self.get_fingerprint_vect(AllChem.MolFromSmiles(smiles))  # Target fingerprint\n",
    "        emb_target = self.compute_embedding_from_fingerprint(fp_target)  # Target embedding\n",
    "        \n",
    "#         emb_purch_molecules = self.compute_embedding_from_fingerprint(self._fps)  # Purchasable molecules embeddings\n",
    "#         print('Embedded purchasable molecules')\n",
    "\n",
    "        # Euclidean (or cosine) distance between embeddings\n",
    "        distances = self.embedding_distance(emb_target, self.emb_purch_molecules)\n",
    "        return torch.min(distances).item()\n",
    "\n",
    "    def _evaluate_nodes(self, nodes: list[OrNode], graph=None) -> list[float]:\n",
    "        if len(nodes) == 0:\n",
    "            return []\n",
    "\n",
    "        # Get distances to nearest neighbours\n",
    "        nn_dists = np.asarray(\n",
    "            [self._get_nearest_neighbour_dist(node.mol.smiles) for node in nodes]\n",
    "        )\n",
    "        assert np.min(nn_dists) >= 0, f'Negative distance: {np.min(nn_dists)} '\n",
    "\n",
    "        # Turn into costs\n",
    "        if self.distance_to_cost == DistanceToCost.NOTHING:\n",
    "            values = nn_dists\n",
    "        elif self.distance_to_cost == DistanceToCost.EXP:\n",
    "            values = np.exp(nn_dists) - 1\n",
    "        elif self.distance_to_cost == DistanceToCost.SQRT:\n",
    "            values = np.sqrt(nn_dists) \n",
    "        elif self.distance_to_cost == DistanceToCost.TIMES10:\n",
    "            values = 10.0*nn_dists\n",
    "        elif self.distance_to_cost == DistanceToCost.TIMES100:\n",
    "            values = 100.0*nn_dists\n",
    "        else:\n",
    "            raise NotImplementedError(self.distance_to_cost)\n",
    "\n",
    "        return list(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bc801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9d9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_div = True # Count number of diverse routes found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ebc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faster implementation\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from syntheseus.search.algorithms.best_first.retro_star import RetroStarSearch\n",
    "from syntheseus.search.graph.and_or import ANDOR_NODE, AndOrGraph, OrNode\n",
    "\n",
    "\n",
    "class ReduceValueFunctionCallsRetroStar(RetroStarSearch):\n",
    "    \"\"\"\n",
    "    More efficient version of Retro* which saves value function calls.\n",
    "    The difference is that retro* calls the value function (i.e. reaction number estimator)\n",
    "    for every leaf node whereas this algorithm assigns a placeholder value of 0 to every leaf node\n",
    "    and only calls the value function if it visits that node a second time.\n",
    "    This essentially leaves the behaviour of retro* unchanged, but saves value function calls.\n",
    "\n",
    "    The reason this works is that retro* greedily expands nodes on the current lowest-cost route,\n",
    "    using the value function (reaction number) estimate as the cost of the node.\n",
    "    If a node is not visited with a value function estimate of 0,\n",
    "    then it would definitely not be visited with a non-zero value function estimate.\n",
    "    Therefore if a node is not visited with a placeholder value of 0,\n",
    "    it doesn't really matter what the value function estimate is.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self, graph: AndOrGraph) -> None:\n",
    "        # If there is only 1 node, \"visit\" it by setting its reaction number estimate to 0\n",
    "        # and incrementing its visit count\n",
    "        if len(graph) == 1:\n",
    "            graph.root_node.num_visit += 1\n",
    "            graph.root_node.data.setdefault(\"reaction_number_estimate\", 0.0)\n",
    "\n",
    "        return super().setup(graph)\n",
    "\n",
    "    def visit_node(self, node: OrNode, graph: AndOrGraph) -> Sequence[ANDOR_NODE]:\n",
    "        \"\"\"\n",
    "        If node.num_visit == 0 then evaluate the value function and return.\n",
    "        Otherwise expand.\n",
    "        \"\"\"\n",
    "        assert node.num_visit >= 0  # should not be negative\n",
    "        node.num_visit += 1\n",
    "        if node.num_visit == 1:\n",
    "            # Evaluate value function and return.\n",
    "            node.data[\"reaction_number_estimate\"] = self.reaction_number_estimator(\n",
    "                [node]\n",
    "            )[0]\n",
    "            return []\n",
    "        else:\n",
    "            return super().visit_node(node, graph)\n",
    "\n",
    "    def _set_reaction_number_estimate(\n",
    "        self, or_nodes: Sequence[OrNode], graph: AndOrGraph\n",
    "    ) -> None:\n",
    "        for node in or_nodes:\n",
    "            node.data.setdefault(\"reaction_number_estimate\", 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9be200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demo script comparing nearest neighbour cost function with constant value function on PaRoutes.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from syntheseus.search.chem import Molecule\n",
    "from syntheseus.search.graph.and_or import AndNode\n",
    "from syntheseus.search.algorithms.best_first.retro_star import RetroStarSearch, MolIsPurchasableCost\n",
    "from syntheseus.search.analysis.solution_time import get_first_solution_time\n",
    "from syntheseus.search.analysis.route_extraction import min_cost_routes\n",
    "from syntheseus.search.reaction_models.base import BackwardReactionModel\n",
    "from syntheseus.search.mol_inventory import BaseMolInventory\n",
    "from syntheseus.search.node_evaluation.base import (\n",
    "    BaseNodeEvaluator,\n",
    "    NoCacheNodeEvaluator,\n",
    ")\n",
    "from syntheseus.search.node_evaluation.common import ConstantNodeEvaluator\n",
    "\n",
    "from paroutes import PaRoutesInventory, PaRoutesModel, get_target_smiles\n",
    "# from neighbour_value_functions import TanimotoNNCostEstimator, DistanceToCost\n",
    "\n",
    "from syntheseus.search.analysis import diversity\n",
    "# from syntheseus.search.algorithms.best_first.retro_star import MolIsPurchasableCost\n",
    "\n",
    "class SearchResult:\n",
    "    def __init__(self, name, soln_time_dict, num_different_routes_dict, \n",
    "                 final_num_rxn_model_calls_dict, final_num_value_function_calls_dict,\n",
    "                 output_graph_dict, routes_dict):\n",
    "        self.name = name\n",
    "        self.soln_time_dict = soln_time_dict\n",
    "        self.num_different_routes_dict = num_different_routes_dict\n",
    "        self.final_num_rxn_model_calls_dict = final_num_rxn_model_calls_dict\n",
    "        self.output_graph_dict = output_graph_dict\n",
    "        self.routes_dict = routes_dict\n",
    "        self.final_num_value_function_calls_dict = final_num_value_function_calls_dict\n",
    "\n",
    "\n",
    "class PaRoutesRxnCost(NoCacheNodeEvaluator[AndNode]):\n",
    "    \"\"\"Cost of reaction is negative log softmax, floored at -3.\"\"\"\n",
    "\n",
    "    def _evaluate_nodes(self, nodes: list[AndNode], graph=None) -> list[float]:\n",
    "        softmaxes = np.asarray([node.reaction.metadata[\"softmax\"] for node in nodes])\n",
    "        costs = np.clip(-np.log(softmaxes), 1e-1, 10.0)\n",
    "        return costs.tolist()\n",
    "\n",
    "\n",
    "def run_algorithm(\n",
    "    name: str,\n",
    "    smiles_list: list[str],\n",
    "    value_function: BaseNodeEvaluator,\n",
    "    rxn_model: BackwardReactionModel,\n",
    "    inventory: BaseMolInventory,\n",
    "    and_node_cost_fn: BaseNodeEvaluator[AndNode],\n",
    "    or_node_cost_fn: BaseNodeEvaluator[OrNode],\n",
    "    max_expansion_depth: int = 15,\n",
    "    prevent_repeat_mol_in_trees: bool= True,\n",
    "    use_tqdm: bool = False,\n",
    "    limit_rxn_model_calls: int = 100,\n",
    "    limit_iterations: int = 1_000_000,\n",
    "    logger: logging.RootLogger = logging.getLogger(),\n",
    ") -> SearchResult:\n",
    "    \"\"\"\n",
    "    Do search on a list of SMILES strings and report the time of first solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize algorithm.\n",
    "    common_kwargs = dict(\n",
    "        reaction_model=rxn_model,\n",
    "        mol_inventory=inventory,\n",
    "        limit_reaction_model_calls=limit_rxn_model_calls,\n",
    "        limit_iterations=limit_iterations,\n",
    "        max_expansion_depth=max_expansion_depth,  # prevent overly-deep solutions\n",
    "        prevent_repeat_mol_in_trees=prevent_repeat_mol_in_trees,  # original paper did this\n",
    "    )\n",
    "    alg = ReduceValueFunctionCallsRetroStar(\n",
    "            and_node_cost_fn=PaRoutesRxnCost(), value_function=value_function, **common_kwargs\n",
    "        )\n",
    "\n",
    "    # Do search\n",
    "    logger.info(f\"Start search with {name}\")\n",
    "    min_soln_times: list[tuple[float, ...]] = []\n",
    "    if use_tqdm:\n",
    "        smiles_iter = tqdm(smiles_list)\n",
    "    else:\n",
    "        smiles_iter = smiles_list\n",
    "        \n",
    "    output_graph_dict = {}\n",
    "    soln_time_dict = {}\n",
    "    routes_dict = {}\n",
    "    final_num_rxn_model_calls_dict = {}\n",
    "    final_num_value_function_calls_dict = {}\n",
    "    num_different_routes_dict = {}\n",
    "    \n",
    "    for i, smiles in enumerate(smiles_iter):\n",
    "        logger.debug(f\"Start search {i}/{len(smiles_list)}. SMILES: {smiles}\")\n",
    "        this_soln_times = list()\n",
    "        alg.reset()\n",
    "        output_graph, _ = alg.run_from_mol(Molecule(smiles))\n",
    "\n",
    "        # Analyze solution time\n",
    "        for node in output_graph.nodes():\n",
    "            node.data[\"analysis_time\"] = node.data[\"num_calls_rxn_model\"]\n",
    "        soln_time = get_first_solution_time(output_graph)\n",
    "        this_soln_times.append(soln_time)\n",
    "\n",
    "        # Analyze number of routes\n",
    "        MAX_ROUTES = 10000\n",
    "        routes = list(min_cost_routes(output_graph, MAX_ROUTES))\n",
    "\n",
    "        if alg.reaction_model.num_calls() < limit_rxn_model_calls:\n",
    "            note = \" (NOTE: this was less than the maximum budget)\"\n",
    "        else:\n",
    "            note = \"\"\n",
    "        logger.debug(\n",
    "            f\"Done {name}: nodes={len(output_graph)}, solution time = {soln_time}, \"\n",
    "            f\"num routes = {len(routes)} (capped at {MAX_ROUTES}), \"\n",
    "            f\"final num rxn model calls = {alg.reaction_model.num_calls()}{note}, \"\n",
    "            f\"final num value model calls = {alg.value_function.num_calls}.\"\n",
    "        )\n",
    "\n",
    "        # Analyze route diversity \n",
    "        if (len(routes)>0) & route_div:\n",
    "            route_objects = [output_graph.to_synthesis_graph(nodes) for nodes in routes]\n",
    "            packing_set = diversity.estimate_packing_number(\n",
    "                routes=route_objects,\n",
    "                distance_metric=diversity.reaction_jaccard_distance,\n",
    "                radius=0.999  # because comparison is > not >=\n",
    "            )\n",
    "            logger.debug((f\"number of distinct routes = {len(packing_set)}\"))\n",
    "        else:\n",
    "            packing_set = []\n",
    "\n",
    "        # Save results\n",
    "        soln_time_dict.update({smiles: soln_time})\n",
    "        final_num_rxn_model_calls_dict.update({smiles: alg.reaction_model.num_calls()})\n",
    "        final_num_value_function_calls_dict.update({smiles: alg.value_function.num_calls})\n",
    "        num_different_routes_dict.update({smiles: len(packing_set)})\n",
    "        output_graph_dict.update({smiles: output_graph})\n",
    "        routes_dict.update({smiles: routes})\n",
    "            \n",
    "    return SearchResult(name=name,\n",
    "                        soln_time_dict=soln_time_dict, \n",
    "                        num_different_routes_dict=num_different_routes_dict, \n",
    "                        final_num_rxn_model_calls_dict=final_num_rxn_model_calls_dict, \n",
    "                        final_num_value_function_calls_dict=final_num_value_function_calls_dict,\n",
    "                        output_graph_dict=output_graph_dict, \n",
    "                        routes_dict=routes_dict)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446ac57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620f1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "\n",
    "# COMMAND LINE\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         \"--limit_num_smiles\",\n",
    "#         type=int,\n",
    "#         default=None,\n",
    "#         help=\"Maximum number of SMILES to run.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--limit_iterations\",\n",
    "#         type=int,\n",
    "#         default=500,\n",
    "#         help=\"Maximum number of algorithm iterations.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--limit_rxn_model_calls\",\n",
    "#         type=int,\n",
    "#         default=25,\n",
    "#         help=\"Allowed number of calls to reaction model.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--paroutes_n\",\n",
    "#         type=int,\n",
    "#         default=5,\n",
    "#         help=\"Which PaRoutes benchmark to use.\",\n",
    "#     )\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "# NOTEBOOK\n",
    "class Args:\n",
    "    limit_num_smiles = 100\n",
    "    limit_iterations = 500 # 100000\n",
    "    limit_rxn_model_calls = 100 # 500\n",
    "    paroutes_n = 5\n",
    "    max_expansion_depth = 20\n",
    "    max_num_templates = 10  # Default 50\n",
    "    prevent_repeat_mol_in_trees = True\n",
    "    rxn_model = 'PAROUTES'\n",
    "    inventory = 'PAROUTES'\n",
    "    and_node_cost_fn='PAROUTES'\n",
    "    or_node_cost_fn = 'MOL_PURCHASABLE' \n",
    "\n",
    "\n",
    "args=Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1981e6a",
   "metadata": {},
   "source": [
    "### Load embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'fingerprints_v1'\n",
    "emb_model_input_folder = f'GraphRuns/{experiment_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59982459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read json.config\n",
    "with open(f'{emb_model_input_folder}/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "# 2. Load model\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class FingerprintModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FingerprintModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# # 2-OPTION 1 - Load from checkpoint (last for now)\n",
    "# if config[\"model_type\"] == 'gnn':\n",
    "#     with open(f'{emb_model_input_folder}/input_dim.pickle', 'rb') as f:\n",
    "#         input_dim_dict = pickle.load(f)\n",
    "#         gnn_input_dim = input_dim_dict['input_dim']\n",
    "# #     gnn_input_dim = preprocessed_targets[0].node_features.shape[1]\n",
    "#     gnn_hidden_dim = config[\"hidden_dim\"]\n",
    "#     gnn_output_dim = config[\"output_dim\"]\n",
    "# elif config[\"model_type\"] == 'fingerprints':\n",
    "#     with open(f'{emb_model_input_folder}/input_dim.pickle', 'rb') as f:\n",
    "#         input_dim_dict = pickle.load(f)\n",
    "#         fingerprint_input_dim = input_dim_dict['input_dim']\n",
    "# #     fingerprint_input_dim = (preprocessed_targets[0].size()[0])\n",
    "#     fingerprint_hidden_dim = config[\"hidden_dim\"]\n",
    "#     fingerprint_output_dim = config[\"output_dim\"]\n",
    "# else:\n",
    "#     raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "\n",
    "# if config[\"model_type\"] == 'gnn':\n",
    "#     model = GNNModel(\n",
    "#         input_dim=gnn_input_dim, \n",
    "#         hidden_dim=gnn_hidden_dim, \n",
    "#         output_dim=gnn_output_dim).to(device)\n",
    "    \n",
    "# elif config[\"model_type\"] == 'fingerprints':\n",
    "#     model = FingerprintModel(\n",
    "#         input_dim=fingerprint_input_dim, \n",
    "#         hidden_dim=fingerprint_hidden_dim, \n",
    "#         output_dim=fingerprint_output_dim).to(device)\n",
    "# else:\n",
    "#     raise NotImplementedError(f'Model type {config[\"model_type\"]}')\n",
    "    \n",
    "    \n",
    "# checkpoint_path = f'{emb_model_input_folder}/epoch_100_checkpoint.pth'\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# # Load the model state dict from the checkpoint\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model_fingerprints_v1 = model\n",
    "\n",
    "# 2-OPTION 2 - Load from pickle (best)\n",
    "model_fingerprints_v1_path = f'{emb_model_input_folder}/model_min_val.pkl'\n",
    "with open(model_fingerprints_v1_path, 'rb') as f:\n",
    "    model_fingerprints_v1 = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e8c5e",
   "metadata": {},
   "source": [
    "## Create dataframe for time to solution and number of routes found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eafb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_type_fingerprints_v1 = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18832768",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Stat initialization Emb\n",
      "End initialization Emb\n",
      "2023-06-27 09:28:35,661 root INFO Start experiment 202306-2611-1616-0f701fe7-bbee-4d2c-83f7-bba18beb858a\n",
      "2023-06-27 09:28:35,662 root INFO Args: \n",
      "and_node_cost_fn: PAROUTES\n",
      "inventory: PAROUTES\n",
      "limit_iterations: 500\n",
      "limit_num_smiles: 100\n",
      "limit_rxn_model_calls: 100\n",
      "max_expansion_depth: 20\n",
      "max_num_templates: 10\n",
      "or_node_cost_fn: MOL_PURCHASABLE\n",
      "paroutes_n: 5\n",
      "prevent_repeat_mol_in_trees: True\n",
      "rxn_model: PAROUTES\n",
      "2023-06-27 09:28:35,662 root INFO dim_test: 100\n",
      "2023-06-27 09:28:35,663 root INFO Start search with Tanimoto-distance-TIMES10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1a0e3379f04a1d8e05e2eb45801451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilariasartori/syntheseus/syntheseus/search/algorithms/best_first/retro_star.py:343: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  parent.data[\"retro_star_value\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 09:39:39,337 root INFO Start search with Tanimoto-distance-TIMES100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c089b675016c4ce5b335f985dd7b49e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 09:50:20,526 root INFO Start search with Tanimoto-distance-EXP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aa43e829384eb1b34a92a0ab9935a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:05:49,158 root INFO Start search with Tanimoto-distance-SQRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1dbdda91734d1290225672d26a3b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:21:09,178 root INFO Start search with Tanimoto-distance-NUM_NEIGHBORS_TO_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0053ae296c47ac8dddfa26ebccf2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:36:39,584 root INFO Start search with Embedding-from-fingerprints-TIMES100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a75b89302d45f8853e5e0428ec2cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.INFO)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(f'{output_folder}/logs.txt', mode='w')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "\n",
    "# Load all SMILES to test\n",
    "# test_smiles = get_target_smiles(args.paroutes_n) # Paroutes\n",
    "\n",
    "# guacamol_file_path = '/scratch/is541/retrosynthesis_dev/Data/Guacamol/guacamol_v1_test_10ksample.txt'\n",
    "guacamol_file_path = 'guacamol_v1_test_10ksample.txt'\n",
    "with open(guacamol_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        test_smiles = [line.strip() for line in lines]  # NOTE: no header\n",
    "\n",
    "\n",
    "if args.limit_num_smiles is not None:\n",
    "    test_smiles = test_smiles[: args.limit_num_smiles]\n",
    "\n",
    "# Make reaction model, inventory, cost functions and value functions\n",
    "if args.and_node_cost_fn == 'PAROUTES':\n",
    "    and_node_cost_fn=PaRoutesRxnCost()\n",
    "else:\n",
    "    raise NotImplementedError(f'and_node_cost_fn: {args.and_node_cost_fn}')\n",
    "\n",
    "if args.or_node_cost_fn == 'MOL_PURCHASABLE':\n",
    "    or_node_cost_fn=MolIsPurchasableCost()\n",
    "else:\n",
    "    raise NotImplementedError(f'or_node_cost_fn: {args.or_node_cost_fn}')\n",
    "\n",
    "if args.inventory == 'PAROUTES':\n",
    "    inventory=PaRoutesInventory(n=args.paroutes_n)\n",
    "else:\n",
    "    raise NotImplementedError(f'inventory: {args.inventory}')\n",
    "\n",
    "if args.rxn_model == 'PAROUTES':\n",
    "    rxn_model=PaRoutesModel(max_num_templates=args.max_num_templates)\n",
    "else:\n",
    "    raise NotImplementedError(f'rxn_model: {args.rxn_model}')\n",
    "\n",
    "\n",
    "value_fns = [\n",
    "#     (\"constant-0\", ConstantNodeEvaluator(0.0)),\n",
    "#     (\n",
    "#         \"Tanimoto-distance\",\n",
    "#         TanimotoNNCostEstimator(\n",
    "#             inventory=inventory, distance_to_cost=DistanceToCost.NOTHING\n",
    "#         ),\n",
    "#     ),\n",
    "    (\n",
    "        \"Tanimoto-distance-TIMES10\",\n",
    "        TanimotoNNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.TIMES10\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Tanimoto-distance-TIMES100\",\n",
    "        TanimotoNNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.TIMES100\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Tanimoto-distance-EXP\",\n",
    "        TanimotoNNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.EXP\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Tanimoto-distance-SQRT\",\n",
    "        TanimotoNNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.SQRT\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Tanimoto-distance-NUM_NEIGHBORS_TO_1\",\n",
    "        TanimotoNNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.NUM_NEIGHBORS_TO_1\n",
    "        ),\n",
    "    ),\n",
    "#     (\n",
    "#         \"Embedding-from-fingerprints\",\n",
    "#         Emb_from_fingerprints_NNCostEstimator(\n",
    "#             inventory=inventory, distance_to_cost=DistanceToCost.NOTHING,\n",
    "#             model=model_fingerprints_v1, distance_type=distance_type_fingerprints_v1,\n",
    "\n",
    "#         ),\n",
    "#     ),\n",
    "#     (\n",
    "#         \"Embedding-from-fingerprints-TIMES10\",\n",
    "#         Emb_from_fingerprints_NNCostEstimator(\n",
    "#             inventory=inventory, distance_to_cost=DistanceToCost.TIMES10,\n",
    "#             model=model_fingerprints_v1, distance_type=distance_type_fingerprints_v1,\n",
    "\n",
    "#         ),\n",
    "#     ),\n",
    "    (\n",
    "        \"Embedding-from-fingerprints-TIMES100\",\n",
    "        Emb_from_fingerprints_NNCostEstimator(\n",
    "            inventory=inventory, distance_to_cost=DistanceToCost.TIMES100,\n",
    "            model=model_fingerprints_v1, distance_type=distance_type_fingerprints_v1,\n",
    "\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "labelalias = {\n",
    "    'constant-0': 'constant-0',\n",
    "    'Tanimoto-distance': 'Tanimoto',\n",
    "    'Tanimoto-distance-TIMES10': 'Tanimoto_times10',\n",
    "    'Tanimoto-distance-TIMES100': 'Tanimoto_times100',\n",
    "    'Tanimoto-distance-EXP': 'Tanimoto_exp',\n",
    "    'Tanimoto-distance-SQRT': 'Tanimoto_sqrt',\n",
    "    \"Tanimoto-distance-NUM_NEIGHBORS_TO_1\": \"Tanimoto_nn_to_1\",\n",
    "    \"Embedding-from-fingerprints\": \"Emb_fnps\",\n",
    "    \"Embedding-from-fingerprints-TIMES10\": \"Emb_fnps_times10\",\n",
    "    \"Embedding-from-fingerprints-TIMES100\": \"Emb_fnps_times10\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Run\n",
    "logger.info(f\"Start experiment {eventid}\")\n",
    "args_string = \"\"\n",
    "for attr in dir(args):\n",
    "    if not callable(getattr(args, attr)) and not attr.startswith(\"__\"):\n",
    "        args_string = args_string + \"\\n\" + (f\"{attr}: {getattr(args, attr)}\") \n",
    "logger.info(f\"Args: {args_string}\")\n",
    "logger.info(f\"dim_test: {len(test_smiles)}\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "### RUN\n",
    "result={}\n",
    "for name, fn in value_fns:\n",
    "    alg_result = run_algorithm(\n",
    "        name=name,\n",
    "        smiles_list=test_smiles, \n",
    "        value_function=fn, \n",
    "        rxn_model=rxn_model,\n",
    "        inventory=inventory,\n",
    "        and_node_cost_fn=and_node_cost_fn,\n",
    "        or_node_cost_fn=or_node_cost_fn, \n",
    "        max_expansion_depth=args.max_expansion_depth, \n",
    "        prevent_repeat_mol_in_trees=args.prevent_repeat_mol_in_trees, \n",
    "        use_tqdm=True,\n",
    "        limit_rxn_model_calls=args.limit_rxn_model_calls, \n",
    "        limit_iterations=args.limit_iterations,\n",
    "        logger=logger,\n",
    "    )\n",
    "    result[name] = alg_result\n",
    "    \n",
    "    # Save pickle\n",
    "    with open(f'{output_folder}/result_{name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(alg_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# ### LOAD result dict from pickles\n",
    "# result = {}\n",
    "# for name, fn in value_fns:\n",
    "#     pickle_name = f'{output_folder}/result_{name}.pickle'\n",
    "#     print(pickle_name)\n",
    "#     with open(pickle_name, 'rb') as handle:\n",
    "#         result[name] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5e60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce35b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def create_result_df(result, name):\n",
    "#     assert name == result[name].name, f\"name: {name} is different from result[name].name: {result[name].name}\"\n",
    "    \n",
    "#     soln_time_dict = result[name].soln_time_dict\n",
    "#     num_different_routes_dict = result[name].num_different_routes_dict\n",
    "#     final_num_rxn_model_calls_dict = result[name].final_num_rxn_model_calls_dict\n",
    "#     output_graph_dict = result[name].output_graph_dict\n",
    "#     routes_dict = result[name].routes_dict\n",
    "\n",
    "#     # df_results = pd.DataFrame()\n",
    "#     df_soln_time = pd.DataFrame({'algorithm': [], 'similes': [], 'property':[], 'value': []})\n",
    "#     df_different_routes = pd.DataFrame({'algorithm': [], 'similes': [], 'property':[], 'value': []})\n",
    "\n",
    "#     #     for name_alg, value_dict  in soln_time_dict.items():\n",
    "#     for smiles, value  in soln_time_dict.items():\n",
    "#         row_soln_time = {'algorithm': name, 'similes': smiles, 'property':'sol_time', 'value': value}\n",
    "\n",
    "#         df_soln_time = pd.concat([df_soln_time, pd.DataFrame([row_soln_time])], ignore_index=True)\n",
    "\n",
    "#     #     for name_alg, value_dict  in num_different_routes_dict.items():\n",
    "#     for smiles, value  in num_different_routes_dict.items():\n",
    "#         row_different_routes = {'algorithm': name, 'similes': smiles, 'property':'diff_routes', 'value': value}\n",
    "\n",
    "#         df_different_routes = pd.concat([df_different_routes, pd.DataFrame([row_different_routes])], ignore_index=True)\n",
    "\n",
    "#     df_results_tot = pd.concat([df_soln_time, df_different_routes], axis=0)\n",
    "#     return df_results_tot\n",
    "\n",
    "\n",
    "\n",
    "# df_results_tot = pd.DataFrame({'algorithm': [], 'similes': [], 'property':[], 'value': []})\n",
    "# for name in tqdm(result.keys()):\n",
    "#     df_results_alg = create_result_df(result, name)\n",
    "#     df_results_tot = pd.concat([df_results_tot, df_results_alg], axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b4c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85028c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to csv\n",
    "# # df_results_tot.to_csv(f'Results/Compare/compare_times_{dim_test}.csv', index=False)\n",
    "# df_results_tot.to_csv(f'{output_folder}/results_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13598b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e78529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7523b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
