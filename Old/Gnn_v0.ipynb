{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "210180cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import deepchem as dc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = 'GraphRuns'\n",
    "# if not os.path.exists(checkpoint_folder):\n",
    "#     os.makedirs(checkpoint_folder)\n",
    "\n",
    "checkpoint_name = 'gnn_checkpoint.pth'\n",
    "\n",
    "not_in_route_sample_size = 10\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cd352",
   "metadata": {},
   "source": [
    "### Read routes data - consider only route 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c34f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '202305-2911-2320-5a95df0e-3008-4ebe-acd8-ecb3b50607c7'\n",
    "\n",
    "input_file_routes = f'Runs/{run_id}/targ_routes.pickle'\n",
    "input_file_distances = f'Runs/{run_id}/targ_to_purch_distances.pickle'\n",
    "\n",
    "# Routes data\n",
    "with open(input_file_routes, 'rb') as handle:\n",
    "    targ_routes_dict = pickle.load(handle)\n",
    "    \n",
    "# Load distances data\n",
    "with open(input_file_distances, 'rb') as handle:\n",
    "    distances_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Inventory\n",
    "from paroutes import PaRoutesInventory\n",
    "inventory=PaRoutesInventory(n=5)\n",
    "purch_smiles = [mol.smiles for mol in inventory.purchasable_mols()]\n",
    "len(purch_smiles)\n",
    "\n",
    "def num_heavy_atoms(mol):\n",
    "    return Chem.rdchem.Mol.GetNumAtoms(mol, onlyExplicit=True)\n",
    "\n",
    "purch_mol_to_exclude = []\n",
    "for smiles in purch_smiles:\n",
    "    if num_heavy_atoms(Chem.MolFromSmiles(smiles)) < 2:\n",
    "        purch_mol_to_exclude = purch_mol_to_exclude + [smiles]\n",
    "\n",
    "\n",
    "\n",
    "targ_route_not_in_route_dict = {}\n",
    "for target, target_routes_dict in targ_routes_dict.items():\n",
    "    targ_route_not_in_route_dict[target] = {}\n",
    "    \n",
    "    target_route_df = target_routes_dict[\"route_1\"]\n",
    "    purch_in_route = list(target_route_df['smiles'])\n",
    "    purch_not_in_route = [purch_smile for purch_smile in purch_smiles if purch_smile not in purch_in_route]\n",
    "    random.seed(seed)\n",
    "    purch_not_in_route_sample = random.sample(purch_not_in_route,not_in_route_sample_size)\n",
    "    \n",
    "    # Filter out molecules with only one atom (problems with featurizer)\n",
    "    purch_in_route = [smiles for smiles in purch_in_route if smiles not in purch_mol_to_exclude]\n",
    "    purch_not_in_route_sample = [smiles for smiles in purch_not_in_route_sample if smiles not in purch_mol_to_exclude]\n",
    "    \n",
    "    targ_route_not_in_route_dict[target]['positive_samples'] = purch_in_route\n",
    "    targ_route_not_in_route_dict[target]['negative_samples'] = purch_not_in_route_sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49573e",
   "metadata": {},
   "source": [
    "### Temp - select a sample of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e572c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of keys from targ_routes_dict\n",
    "nr_sample_targets = 200\n",
    "sample_targets = random.sample(list(targ_route_not_in_route_dict.keys()), nr_sample_targets)\n",
    "\n",
    "# Create targ_routes_dict_sample with the sampled keys and their corresponding values\n",
    "targ_route_not_in_route_dict_sample = {target: targ_route_not_in_route_dict[target] for target in sample_targets}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da030ef2",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b650e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleData:\n",
    "    def __init__(self, target, positive_samples, negative_samples):\n",
    "        self.target = target\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "\n",
    "# INPUT\n",
    "\n",
    "# OUTPUT \n",
    "# data = [\n",
    "#     {\n",
    "#         'target': 'target1_feats',\n",
    "#         'positive_samples': ['positive1_feats', 'positive2_feats'],\n",
    "#         'negative_samples': ['negative1_feats', 'negative2_feats']\n",
    "#     },\n",
    "#     {\n",
    "#         'target': 'target2_feats',\n",
    "#         'positive_samples': ['positive3_feats', 'positive4_feats']\n",
    "#         'negative_samples': ['negative3_feats', 'negative4_feats']\n",
    "#     },\n",
    "#     # ... add more data samples\n",
    "# ]\n",
    "def preprocess_input(input_data):\n",
    "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "    data_list = []\n",
    "    \n",
    "    for target_smiles, samples in tqdm(input_data.items()):\n",
    "        target_feats = featurizer.featurize(Chem.MolFromSmiles(target_smiles))\n",
    "        pos_mols = [Chem.MolFromSmiles(positive_smiles) for positive_smiles in samples['positive_samples']]\n",
    "        neg_mols = [Chem.MolFromSmiles(negative_smiles) for negative_smiles in samples['negative_samples']]\n",
    "        pos_feats = featurizer.featurize(pos_mols)\n",
    "        neg_feats = featurizer.featurize(neg_mols)\n",
    "        data_list = data_list + [SampleData(target=target_feats, positive_samples=pos_feats, negative_samples=neg_feats)]\n",
    "    return data_list\n",
    "        \n",
    "#     X = []\n",
    "#     y = []\n",
    "#     featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "#     for target_smiles, samples in tqdm(input_data.items()):\n",
    "#         target_label = 1.0  # Assign a positive label to the target molecule\n",
    "#         target_mol = Chem.MolFromSmiles(target_smiles)\n",
    "\n",
    "#         # Add positive samples to the dataset\n",
    "#         for positive_smiles in samples['positive_samples']:\n",
    "#             positive_mol = Chem.MolFromSmiles(positive_smiles)\n",
    "#             target_positive_feats = featurizer.featurize([target_mol, positive_mol])\n",
    "#             X.append(target_positive_feats)\n",
    "#             y.append(target_label)\n",
    "\n",
    "#         # Add negative samples to the dataset\n",
    "#         for negative_smiles in samples['negative_samples']:\n",
    "#             negative_mol = Chem.MolFromSmiles(negative_smiles)\n",
    "#             target_negative_feats = featurizer.featurize([target_mol, negative_mol])\n",
    "#             X.append(target_negative_feats)\n",
    "#             y.append(0.0)  # Assign a negative label\n",
    "\n",
    "#     X = np.concatenate(X, axis=0)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     dataset = dc.data.NumpyDataset(X, y)\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# def preprocess_input(input_data):\n",
    "#     data_list = []\n",
    "    \n",
    "#     for target, samples in input_data.items():\n",
    "#         target_embedding = get_embedding(target)  # Assuming you have a function to obtain the embedding for a target molecule\n",
    "        \n",
    "#         for positive_sample in samples['positive_samples']:\n",
    "#             positive_embedding = get_embedding(positive_sample)  # Assuming you have a function to obtain the embedding for a positive sample\n",
    "            \n",
    "#             positive_data = Data(x=positive_embedding, y=torch.tensor([1.0]))  # Create a positive data instance\n",
    "#             data_list.append(positive_data)\n",
    "        \n",
    "#         for negative_sample in samples['negative_samples']:\n",
    "#             negative_embedding = get_embedding(negative_sample)  # Assuming you have a function to obtain the embedding for a negative sample\n",
    "            \n",
    "#             negative_data = Data(x=negative_embedding, y=torch.tensor([0.0]))  # Create a negative data instance\n",
    "#             data_list.append(negative_data)\n",
    "    \n",
    "#     return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea6e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         target_smiles = list(self.data.keys())[idx]\n",
    "#         sample = self.data[target_smiles]\n",
    "\n",
    "#         # Extract the positive_samples and negative_samples\n",
    "#         positive_samples = sample['positive_samples']\n",
    "#         negative_samples = sample['negative_samples']\n",
    "\n",
    "#         # Convert the data to tensors or any other necessary preprocessing\n",
    "\n",
    "#         # Return the sample with named attributes\n",
    "#         return {\n",
    "#             'target_smiles': target_smiles,\n",
    "#             'positive_samples': positive_samples,\n",
    "#             'negative_samples': negative_samples\n",
    "#         }\n",
    "    \n",
    "# Takes a list of samples, where each sample is a dictionary containing the 'target_smiles', \n",
    "# 'positive_sample', and 'negative_samples'. \n",
    "# In the __getitem__ method, we extract these attributes and return the sample as a dictionary \n",
    "# with named attributes.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "#         return sample\n",
    "\n",
    "#         # Extract the target_smiles, positive_sample, and negative_samples\n",
    "# #         target_smiles = sample['target_smiles']\n",
    "# #         positive_sample = sample['positive_sample']\n",
    "# #         negative_samples = sample['negative_samples']\n",
    "        target = sample.target\n",
    "        positive_samples = sample.positive_samples\n",
    "        negative_samples = sample.negative_samples\n",
    "\n",
    "        # Convert the data to tensors or any other necessary preprocessing\n",
    "\n",
    "        # Return the sample with named attributes\n",
    "#         return {\n",
    "#             'target': target,\n",
    "#             'positive_samples': positive_samples,\n",
    "#             'negative_samples': negative_samples\n",
    "#         }\n",
    "        return target, positive_samples, negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a799c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define embedding model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# class GNNModel(MessagePassing):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(GNNModel, self).__init__(aggr='mean')\n",
    "#         self.gnn = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.gnn(x)\n",
    "#         x = self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "#         return x\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#     def __init__(self, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "#         self.conv1 = GCNConv(None,hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Step 3: Create a contrastive learning loss function\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "#     def forward(self, embeddings, positive_pairs):\n",
    "#         anchor, positive = positive_pairs[:, 0], positive_pairs[:, 1]\n",
    "#         anchor_emb = embeddings[anchor]\n",
    "#         positive_emb = embeddings[positive]\n",
    "#         anchor_similarity = self.cos_sim(anchor_emb, positive_emb)\n",
    "#         anchor_similarity /= self.temperature\n",
    "\n",
    "#         all_similarity = self.cos_sim(embeddings.unsqueeze(1), embeddings.unsqueeze(0))\n",
    "#         all_similarity = all_similarity.view(-1)\n",
    "#         all_similarity = all_similarity[~positive_pairs.byte()]\n",
    "\n",
    "#         numerator = torch.exp(anchor_similarity)\n",
    "#         denominator = torch.sum(torch.exp(all_similarity))\n",
    "#         loss = -torch.log(numerator / (numerator + denominator))\n",
    "#         return loss.mean()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        sample_losses = []\n",
    "        for single_sample_embeddings in embeddings:\n",
    "            target_emb = single_sample_embeddings.target\n",
    "            positive_embs = single_sample_embeddings.positive_samples\n",
    "            negative_embs = single_sample_embeddings.negative_samples\n",
    "            \n",
    "            # Sample one positive\n",
    "            nr_positives = len(positive_embs)\n",
    "            positive_emb = sample(positive_embs, 1)\n",
    "            \n",
    "            positive_similarity = self.cos_sim(target_emb, positive_emb)\n",
    "            positive_similarity /= self.temperature\n",
    "            negative_similarity = self.cos_sim(target_emb, negative_embs)\n",
    "            negative_similarity /= self.temperature\n",
    "            \n",
    "            numerator = torch.exp(positive_similarity)\n",
    "            denominator = torch.sum(torch.exp(negative_similarity))\n",
    "            sample_loss = -torch.log(numerator / (numerator + denominator))\n",
    "            sample_losses = sample_losses + [sample_loss]\n",
    "        \n",
    "        return sample_losses.mean()\n",
    "    \n",
    "    \n",
    "# import torch\n",
    "# from pytorch_metric_learning.losses import GenericPairLoss\n",
    "\n",
    "# class NTXentLoss(GenericPairLoss):\n",
    "\n",
    "#     def __init__(self, temperature, **kwargs):\n",
    "#         super().__init__(use_similarity=True, mat_based_loss=False, **kwargs)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def _compute_loss(self, pos_pairs, neg_pairs, indices_tuple):\n",
    "#         a1, p, a2, _ = indices_tuple\n",
    "\n",
    "#         if len(a1) > 0 and len(a2) > 0:\n",
    "#             pos_pairs = pos_pairs.unsqueeze(1) / self.temperature\n",
    "#             neg_pairs = neg_pairs / self.temperature\n",
    "#             n_per_p = (a2.unsqueeze(0) == a1.unsqueeze(1)).float()\n",
    "#             neg_pairs = neg_pairs*n_per_p\n",
    "#             neg_pairs[n_per_p==0] = float('-inf')\n",
    "\n",
    "#             max_val = torch.max(pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0].half()) ###This is the line change\n",
    "#             numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "#             denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1) + numerator\n",
    "#             log_exp = torch.log((numerator/denominator) + 1e-20)\n",
    "#             return {\"loss\": {\"losses\": -log_exp, \"indices\": (a1, p), \"reduction_type\": \"pos_pair\"}}\n",
    "#         return self.zero_losses()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Or use NTXentMultiplePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2624c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 200/200 [01:23<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Prepare the dataset\n",
    "# # Assuming we have a dataset file named 'molecules.csv' with molecular structures and labels\n",
    "# dataset = dc.data.CSVLoader(tasks=['property'], feature_field='smiles')\n",
    "# dataset.load_from_file('molecules.csv')\n",
    "# splitter = dc.splits.RandomSplitter()\n",
    "# train_dataset, valid_dataset, _ = splitter.train_valid_test_split(dataset)\n",
    "\n",
    "# Step 1: Create data dictionary getting negative samples for each target\n",
    "input_data = targ_route_not_in_route_dict_sample\n",
    "\n",
    "# Step 2: Featurizer and cast as CustomDataset\n",
    "preprocessed_data = preprocess_input(input_data)\n",
    "dataset = CustomDataset(preprocessed_data)\n",
    "\n",
    "# Step 3: Create DataLoader\n",
    "# def collate_fn(data):\n",
    "#     targets = [sample[0] for sample in data]\n",
    "#     positive_samples = [sample[1] for sample in data]\n",
    "#     negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "#     return targets, positive_samples, negative_samples\n",
    "def collate_fn(data):\n",
    "    targets = [sample[0] for sample in data]\n",
    "    positive_samples = [sample[1] for sample in data]\n",
    "    negative_samples = [sample[2] for sample in data]\n",
    "\n",
    "    return targets, positive_samples, negative_samples\n",
    "\n",
    "batch_size = 64  \n",
    "shuffle = True  \n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80995894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = CustomDataset(preprocessed_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_input_dim = preprocessed_data[0].target[0].node_features.shape[1]\n",
    "gnn_hidden_dim = 512\n",
    "gnn_output_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab3a9c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([GraphData(node_features=[20, 30], edge_index=[2, 42], edge_features=None, pos=[0])],\n",
       "       dtype=object),\n",
       " array([GraphData(node_features=[20, 30], edge_index=[2, 42], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[5, 30], edge_index=[2, 8], edge_features=None, pos=[0])],\n",
       "       dtype=object),\n",
       " array([GraphData(node_features=[18, 30], edge_index=[2, 36], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 14], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[20, 30], edge_index=[2, 42], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 14], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[21, 30], edge_index=[2, 46], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[17, 30], edge_index=[2, 36], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[19, 30], edge_index=[2, 36], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[6, 30], edge_index=[2, 10], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 16], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[32, 30], edge_index=[2, 70], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 16], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[13, 30], edge_index=[2, 26], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[16, 30], edge_index=[2, 34], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[19, 30], edge_index=[2, 38], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[4, 30], edge_index=[2, 6], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 26], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 12], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[16, 30], edge_index=[2, 34], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 12], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[26, 30], edge_index=[2, 56], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[16, 30], edge_index=[2, 32], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[34, 30], edge_index=[2, 72], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[6, 30], edge_index=[2, 12], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[28, 30], edge_index=[2, 62], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 16], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[10, 30], edge_index=[2, 20], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[10, 30], edge_index=[2, 20], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[19, 30], edge_index=[2, 38], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[6, 30], edge_index=[2, 10], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 12], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[5, 30], edge_index=[2, 8], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[10, 30], edge_index=[2, 20], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[19, 30], edge_index=[2, 40], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[21, 30], edge_index=[2, 46], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[15, 30], edge_index=[2, 32], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[24, 30], edge_index=[2, 52], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 14], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 26], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[3, 30], edge_index=[2, 4], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 16], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 20], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[15, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[6, 30], edge_index=[2, 10], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[5, 30], edge_index=[2, 10], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[19, 30], edge_index=[2, 40], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[18, 30], edge_index=[2, 40], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[23, 30], edge_index=[2, 48], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[16, 30], edge_index=[2, 32], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[7, 30], edge_index=[2, 12], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 32], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[24, 30], edge_index=[2, 52], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[10, 30], edge_index=[2, 20], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[27, 30], edge_index=[2, 58], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[13, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[13, 30], edge_index=[2, 26], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[23, 30], edge_index=[2, 48], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[8, 30], edge_index=[2, 16], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[4, 30], edge_index=[2, 6], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[15, 30], edge_index=[2, 34], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[13, 30], edge_index=[2, 28], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[20, 30], edge_index=[2, 44], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "        GraphData(node_features=[16, 30], edge_index=[2, 34], edge_features=None, pos=[0])],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78fef703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([GraphData(node_features=[20, 30], edge_index=[2, 42], edge_features=None, pos=[0]),\n",
       "       GraphData(node_features=[14, 30], edge_index=[2, 30], edge_features=None, pos=[0]),\n",
       "       GraphData(node_features=[12, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "       GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None, pos=[0]),\n",
       "       GraphData(node_features=[5, 30], edge_index=[2, 8], edge_features=None, pos=[0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[0].positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3165193",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_train_data_loader = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a6f88a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2d70764a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "653842f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     31\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gnn_train_data_loader):\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniforge3/envs/syntheseus_temp_molclr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:169\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "# Step 3: Set up the training loop for the GNN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn_model = GNNModel(\n",
    "    input_dim=gnn_input_dim, \n",
    "    hidden_dim=gnn_hidden_dim, \n",
    "    output_dim=gnn_output_dim).to(device)\n",
    "# gnn_model = GNNModel(hidden_dim=gnn_hidden_dim, output_dim=gnn_output_dim).to(device)\n",
    "\n",
    "gnn_loss_fn = NTXentLoss(temperature=0.1)\n",
    "gnn_optimizer = optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "checkpoint_path = f'{checkpoint_folder}/{checkpoint_name}'  # Specify the checkpoint file path\n",
    "\n",
    "# Check if a checkpoint exists and load the model state and optimizer state if available\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    gnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gnn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create a SummaryWriter for TensorBoard logging\n",
    "log_dir = f'{checkpoint_folder}/logs'  # Specify the directory to store TensorBoard logs\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "gnn_model.train()\n",
    "for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for batch_idx, batch_data in enumerate(gnn_train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # TODO change, scan trough 3 list at the same time (targets, pos, neg)\n",
    "        embeddings = []\n",
    "        for sample_data in batch_data:\n",
    "            target_embedding = gnn_model(sample_data.target)\n",
    "            positive_sample_embeddings = gnn_model(sample_data.positive_samples)\n",
    "            negative_samples_embeddings = gnn_model(sample_data.negative_samples)\n",
    "            embeddings = embeddings + [SampleData(target=target_embedding, positive_samples=positive_sample_embeddings, negative_samples=negative_samples_embeddings)]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = gnn_loss_fn(embeddings)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total loss\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    average_loss = total_loss / total_batches\n",
    "\n",
    "    print(f\"GNN Model - Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Log the loss to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch+1)\n",
    "\n",
    "    # Save the model and optimizer state as a checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': gnn_model.state_dict(),\n",
    "        'optimizer_state_dict': gnn_optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Step 4: Evaluate and use the trained embeddings\n",
    "# You can evaluate the embeddings on downstream tasks or use them for molecular similarity search or property prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626855d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
